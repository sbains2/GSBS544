{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: GSBS544 Lab 8\n",
        "author: Sahil Bains\n",
        "format:\n",
        "  html:\n",
        "    embed-resources: true\n",
        "    code-fold: true\n",
        "echo: true\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Github Link: https://github.com/sbains2/GSBS544\n",
        "\n",
        "Instructions\n",
        "You will submit an HTML document to Canvas as your final version.\n",
        "\n",
        "Your document should show your code chunks/cells as well as any output. Make sure that only relevant output is printed. Do not, for example, print the entire dataset in your final rendered file.\n",
        "\n",
        "Your document should also be clearly organized, so that it is easy for a reader to find your answers to each question.\n",
        "\n",
        "The Data\n",
        "This week, we consider a dataset generated from text data.\n",
        "\n",
        "The original dataset can be found here: https://www.kaggle.com/datasets/kingburrito666/cannabis-strains. It consists of user reviews of different strains of cannabis. Users rated their experience with the cannabis strain on a scale of 1 to 5. They also selected words from a long list to describe the Effects and the Flavor of the cannabis.\n",
        "\n",
        "In the dataset linked above, each row is one strain of cannabis. The average rating of all testers is reported, as well as the most commonly used words for the effect and flavor.\n",
        "\n",
        "Some data cleaning has been performed for you: The Effect and Flavor columns have been converted to dummy variables indicating if the particular word was used for the particular strain.\n",
        "\n",
        "This cleaned data can be found at: https://www.dropbox.com/s/s2a1uoiegitupjc/cannabis_full.csv\n",
        "\n",
        "Our goal will be to fit models that identify the Sativa types from the Indica types, and then to fit models that also distinguish the Hybrid types.\n",
        "\n",
        "IMPORTANT: In this assignment, you do not need to consider different feature sets. Normally, this would be a good thing to try - but for this homework, simply include all the predictors for every model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Importing all necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from plotnine import *\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Reading in the data\n",
        "df = pd.read_csv('weed.csv')\n",
        "# Dropping NaNs\n",
        "weed = df.dropna()\n",
        "weed.info()\n",
        "weed.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Part One: Binary Classification\n",
        "Create a dataset that is limited only to the Sativa and Indica type cannabis strains.\n",
        "\n",
        "This section asks you to create a final best model for each of the four new model types studied this week: LDA, QDA, SVC, and SVM. For SVM, you may limit yourself to only the polynomial kernel.\n",
        "\n",
        "For each, you should:\n",
        "\n",
        "Choose a metric you will use to select your model, and briefly justify your choice. (Hint: There is no specific target category here, so this should not be a metric that only prioritizes one category.)\n",
        "\n",
        "Find the best model for predicting the Type variable. Don’t forget to tune any hyperparameters.\n",
        "\n",
        "Report the (cross-validated!) metric.\n",
        "\n",
        "Fit the final model.\n",
        "\n",
        "Output a confusion matrix.\n",
        "\n",
        "Q1: LDA\n",
        "Q2: QDA\n",
        "Q3: SVC\n",
        "Q4: SVM\n",
        "\n",
        "Creating a dataset with targets Indica and Sativa"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Subsetting the dataset with limiting targets to Indica and Sativa\n",
        "weed = weed[(weed['Type'] == 'indica') | (weed['Type'] == 'sativa')]\n",
        "\n",
        "# Checking that there are only two types in the Type column\n",
        "weed['Type'].nunique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Basic model setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Creating target and explanatory variables\n",
        "X = weed.drop(columns=['Type'])\n",
        "y = weed['Type']\n",
        "\n",
        "# Getting numeric and categorical variables\n",
        "num = X.select_dtypes(include=['number']).columns\n",
        "cat = X.select_dtypes(exclude=['number']).columns\n",
        "\n",
        "# Defining columnTransformer for preprocessing steps\n",
        "ct = ColumnTransformer(\n",
        "    transformers=[\n",
        "    ('num', StandardScaler(), num),\n",
        "    ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), cat)\n",
        "])\n",
        "\n",
        "# Doing a test-train split to partition training and testing data\n",
        "Xt, Xv, yt, yv = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For LDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import math\n",
        "# Creating a pipeline for Linear Discriminatory Analysis\n",
        "LDApipe = Pipeline([\n",
        "    ('preprocess', ct),\n",
        "    # adding param solver='lsqr' to mitigate issue when predictors exceeds the number of samples per class,\n",
        "    # using least squares formulation instead of SVD inversion so that the model tolerates high-dimensional\n",
        "    # highly correlated feature sets.\n",
        "    ('model', LinearDiscriminantAnalysis(solver='lsqr'))\n",
        "])\n",
        "\n",
        "# Fitting the pipe on the training data\n",
        "LDApipe.fit(Xt, yt)\n",
        "\n",
        "# Finding cross-validated accuracy\n",
        "print(np.mean(cross_val_score(LDApipe, X, y, cv=5)))\n",
        "\n",
        "# Generating predictions on the validation set\n",
        "y_pred = LDApipe.predict(Xv)\n",
        "\n",
        "# specifying class order for rows/columns\n",
        "labels = ['indica', 'sativa']\n",
        "\n",
        "# Designing a confusion matrix\n",
        "cm = confusion_matrix(yv, y_pred, labels=labels)\n",
        "cm_df = pd.DataFrame(cm, index=[f\"Actual {lbl}\" for lbl in labels], # appending row labels\n",
        "                    columns=[f\"Predicted {lbl}\" for lbl in labels]) # appending column labels\n",
        "print(cm_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For the LDA model, we find that the mean cross validated metric is 0.54\n",
        "\n",
        "\n",
        "For QDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import math\n",
        "# Creating a pipeline for Quadratic Discriminatory Analysis\n",
        "QDApipe = Pipeline([\n",
        "    ('preprocess', ct),\n",
        "    ('model', QuadraticDiscriminantAnalysis())\n",
        "])\n",
        "\n",
        "# Fitting the pipe on the training data\n",
        "QDApipe.fit(Xt, yt)\n",
        "\n",
        "# Finding cross-validated accuracy\n",
        "print(np.mean(cross_val_score(QDApipe, X, y, cv=5)))\n",
        "\n",
        "# Generating predictions on the validation set\n",
        "y_pred = QDApipe.predict(Xv)\n",
        "\n",
        "# specifying class order for rows/columns\n",
        "labels = ['indica', 'sativa']\n",
        "\n",
        "# Designing a confusion matrix\n",
        "cm = confusion_matrix(yv, y_pred, labels=labels)\n",
        "cm_df = pd.DataFrame(cm, index=[f\"Actual {lbl}\" for lbl in labels], # appending row labels\n",
        "                    columns=[f\"Predicted {lbl}\" for lbl in labels]) # appending column labels\n",
        "print(cm_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "cross validated accuracy: 0.34362550836791794\n",
        "\n",
        "\n",
        "For SVC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Creating a pipeline for Support Vector Classifier\n",
        "SVCpipe = Pipeline([\n",
        "    ('preprocess', ct),\n",
        "    ('model', SVC())\n",
        "])\n",
        "\n",
        "# Fitting the pipe on the training data\n",
        "SVCpipe.fit(Xt, yt)\n",
        "\n",
        "# Finding cross-validated accuracy\n",
        "print(np.mean(cross_val_score(SVCpipe, X, y, cv=5)))\n",
        "\n",
        "# Generating predictions on the validation set\n",
        "y_pred = SVCpipe.predict(Xv)\n",
        "\n",
        "# specifying class order for rows/columns\n",
        "labels = ['indica', 'sativa']\n",
        "\n",
        "# Designing a confusion matrix\n",
        "cm = confusion_matrix(yv, y_pred, labels=labels)\n",
        "cm_df = pd.DataFrame(cm, index=[f\"Actual {lbl}\" for lbl in labels], # appending row labels\n",
        "                    columns=[f\"Predicted {lbl}\" for lbl in labels]) # appending column labels\n",
        "print(cm_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Creating a pipeline for SVM\n",
        "SVMpipe = Pipeline([\n",
        "    ('preprocess', ct),\n",
        "    ('model', SVC(kernel='poly'))\n",
        "])\n",
        "\n",
        "# Tuning key hyperparams for the polynomial kernel\n",
        "svm_params = {\n",
        "    'model__C': [0.1, 1, 10],\n",
        "    'model__degree': [2,3,4],\n",
        "    'model__gamma': ['scale', 'auto'],\n",
        "    'model__coef0': [0,1]\n",
        "}\n",
        "\n",
        "svm_grid = GridSearchCV(SVMpipe, svm_params, cv=5)\n",
        "svm_grid.fit(Xt, yt)\n",
        "\n",
        "# Cross validated accuracy for the best polynomial SVM\n",
        "print(svm_grid.best_score_)\n",
        "print(svm_grid.best_params_)\n",
        "\n",
        "# Generating predictions on the validation set using tuned model\n",
        "best_svm = svm_grid.best_estimator_\n",
        "y_pred = best_svm.predict(Xv)\n",
        "\n",
        "# specifying class order for rows/columns\n",
        "labels = ['indica', 'sativa']\n",
        "\n",
        "# Designing a confusion matrix\n",
        "cm = confusion_matrix(yv, y_pred, labels=labels)\n",
        "cm_df = pd.DataFrame(cm, index=[f\"Actual {lbl}\" for lbl in labels], # appending row labels\n",
        "                    columns=[f\"Predicted {lbl}\" for lbl in labels]) # appending column labels\n",
        "print(cm_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For the SVM model, we find that the mean cross validated metric is 0.858\n",
        "\n",
        "\n",
        "Part Two: Natural Multiclass\n",
        "Now use the full dataset, including the Hybrid strains.\n",
        "\n",
        "Q1\n",
        "Fit a decision tree, plot the final fit, and interpret the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "p2 = df.dropna()\n",
        "\n",
        "# Creating new target and explanatory variables\n",
        "X = p2.drop(columns=['Type'])\n",
        "y = p2['Type']\n",
        "\n",
        "Xt, Xv, yt, yv = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Creating a pipeline for DTC\n",
        "tree = Pipeline([\n",
        "    ('preprocess', ct),\n",
        "    ('model', DecisionTreeClassifier())\n",
        "])\n",
        "\n",
        "tree_grid = {\n",
        "    'model__max_depth': [3,5,7,9,11],\n",
        "    'model__min_samples_split': [2,3,4],\n",
        "    'model__criterion': ['gini', 'entropy']\n",
        "}\n",
        "\n",
        "grid_tree = GridSearchCV(tree, tree_grid, cv=5, scoring='accuracy')\n",
        "grid_tree.fit(Xt, yt)\n",
        "y_pred = grid_tree.predict(Xv)\n",
        "print('tree accuracy', accuracy_score(yv, y_pred))\n",
        "print('classification report', classification_report(yv, y_pred))\n",
        "\n",
        "# plot the tuned tree\n",
        "\n",
        "# getting tuned decision tree\n",
        "best_tree = grid_tree.best_estimator_\n",
        "\n",
        "# extracting importance scores\n",
        "imp = best_tree.named_steps['model'].feature_importances_\n",
        "\n",
        "# extracting feature names after preprocessing\n",
        "feat = best_tree.named_steps['preprocess'].get_feature_names_out()\n",
        "\n",
        "# Building a dataframe of top important feature for plotting\n",
        "formatted = (\n",
        "    pd.DataFrame({'feature': feat, 'importance': imp})\n",
        "                .sort_values('importance', ascending=False)\n",
        "                .head(10) # Limiting to 15 entries\n",
        ")\n",
        "\n",
        "# Plotting\n",
        "(\n",
        "    ggplot(formatted, aes(x='reorder(feature, importance)', y='importance'))\n",
        "    + geom_col()\n",
        "    + coord_flip()\n",
        "    + labs(\n",
        "        x='feature',\n",
        "        y='importance',\n",
        "        title='Decision Tree Feature Importances (top 10)'\n",
        "    )\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From interpreting the plot of the fitted Decision Tree Classifier, we're able to see that the top split drivers are 'sleepy', 'energetic', 'relaxed', 'citrus', 'uplifted', and 'rating', with 'sleepy' accountign for most of the impurity reduction (~0.78).\n",
        "\n",
        "\n",
        "Q2\n",
        "Repeat the analyses from Part One for LDA, QDA, and KNN."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# For LDA\n",
        "# Creating a pipeline for Linear Discriminatory Analysis\n",
        "LDApipe = Pipeline([\n",
        "    ('preprocess', ct),\n",
        "    # adding param solver='lsqr' to mitigate issue when predictors exceeds the number of samples per class,\n",
        "    # using least squares formulation instead of SVD inversion so that the model tolerates high-dimensional\n",
        "    # highly correlated feature sets.\n",
        "    ('model', LinearDiscriminantAnalysis(solver='lsqr'))\n",
        "])\n",
        "\n",
        "# Fitting the pipe on the training data\n",
        "LDApipe.fit(Xt, yt)\n",
        "\n",
        "# Finding cross-validated accuracy\n",
        "print(np.mean(cross_val_score(LDApipe, X, y, cv=5, scoring='accuracy')))\n",
        "\n",
        "# Generating predictions on the validation set\n",
        "y_pred = LDApipe.predict(Xv)\n",
        "\n",
        "# specifying class order for rows/columns\n",
        "labels = ['indica', 'sativa', 'hybrid']\n",
        "\n",
        "# Designing a confusion matrix\n",
        "cm = confusion_matrix(yv, y_pred, labels=labels)\n",
        "cm_df = pd.DataFrame(cm, index=[f\"Actual {lbl}\" for lbl in labels], # appending row labels\n",
        "                    columns=[f\"Predicted {lbl}\" for lbl in labels]) # appending column labels\n",
        "print(cm_df)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "cross validated accuracy: 0.3094756659489708"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# For QDA\n",
        "# Creating a pipeline for Quadratic Discriminatory Analysis\n",
        "QDApipe = Pipeline([\n",
        "    ('preprocess', ct),\n",
        "    ('model', QuadraticDiscriminantAnalysis())\n",
        "])\n",
        "\n",
        "# Fitting the pipe on the training data\n",
        "QDApipe.fit(Xt, yt)\n",
        "\n",
        "# Finding cross-validated accuracy\n",
        "print(np.mean(cross_val_score(QDApipe, X, y, cv=5)))\n",
        "\n",
        "# Generating predictions on the validation set\n",
        "y_pred = QDApipe.predict(Xv)\n",
        "\n",
        "# specifying class order for rows/columns\n",
        "labels = ['indica', 'sativa', 'hybrid']\n",
        "\n",
        "# Designing a confusion matrix\n",
        "cm = confusion_matrix(yv, y_pred, labels=labels)\n",
        "cm_df = pd.DataFrame(cm, index=[f\"Actual {lbl}\" for lbl in labels], # appending row labels\n",
        "                    columns=[f\"Predicted {lbl}\" for lbl in labels]) # appending column labels\n",
        "print(cm_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "cross validated accuracy: 0.34362550836791794"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# For kNN\n",
        "knnpipe = Pipeline([\n",
        "    ('preprocess', ct),\n",
        "    ('model', KNeighborsClassifier())\n",
        "])\n",
        "\n",
        "knnpipe.fit(Xt,yt)\n",
        "\n",
        "knn_grid = {\n",
        "    'model__n_neighbors': [3,5,7,9],\n",
        "    'model__weights': ['uniform', 'distance']\n",
        "}\n",
        "\n",
        "gridknn = GridSearchCV(knnpipe, knn_grid, cv=5, scoring='accuracy')\n",
        "gridknn.fit(Xt, yt)\n",
        "y_pred = gridknn.predict(Xv)\n",
        "\n",
        "\n",
        "print('best score:', gridknn.best_score_)\n",
        "print('knn best params:', gridknn.best_params_)\n",
        "\n",
        "labels = ['indica', 'sativa', 'hybrid']\n",
        "\n",
        "cm = confusion_matrix(yv, y_pred, labels=labels)\n",
        "cm_df = pd.DataFrame(cm, index=[f\"Actual {lbl}\" for lbl in labels], # appending row labels\n",
        "                    columns=[f\"Predicted {lbl}\" for lbl in labels]) # appending column labels\n",
        "print(cm_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "cross validated accuracy: 0.5690565730565731\n",
        "\n",
        "Q3\n",
        "Were your metrics better or worse than in Part One? Why? Which categories were most likely to get mixed up, according to the confusion matrices? Why?\n",
        "\n",
        "In part 2, all 3 models lost accuracy compared to Part 1 because Hybrid overlaps Indica/Sativa, which shrinks the margins. LDA held up the best, while QDA decreased significantly because full covariances get unstable with lots of correlated dummies. kNN landed in between, with Hybrid points often pulled toward nearby Indica and Sativa. Overall,most mistakes were hybrids being predicted as indica or sativa. Indica vs. Sativa stayed cleaner. Because hybrids overlap both groups, accuracy drops overall. With simpler boundaries in LDA, it was able to handle the overlap better than the more flexible models like kNN or QDA. \n",
        "\n",
        "Part Three: Multiclass from Binary\n",
        "Consider two models designed for binary classification: SVC and Logistic Regression.\n",
        "\n",
        "Q1\n",
        "Fit and report metrics for OvR versions of the models. That is, for each of the two model types, create three models:\n",
        "\n",
        "Indica vs. Not Indica"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# For SVC\n",
        "base = df.dropna()\n",
        "tmp = base.copy()\n",
        "tmp['indica'] = (tmp['Type'] == 'indica')\n",
        "\n",
        "X = tmp.drop(columns=['Type', 'indica'])\n",
        "y = tmp['indica']\n",
        "\n",
        "Xt, Xv, yt, yv = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Getting numeric and categorical variables\n",
        "num = X.select_dtypes(include=['number']).columns\n",
        "cat = X.select_dtypes(exclude=['number']).columns\n",
        "\n",
        "# Defining columnTransformer for preprocessing steps\n",
        "ct = ColumnTransformer(\n",
        "    transformers=[\n",
        "    ('num', StandardScaler(), num),\n",
        "    ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), cat)\n",
        "])\n",
        "\n",
        "SVCpipe = Pipeline([\n",
        "    ('preprocess', ct),\n",
        "    ('model', SVC())\n",
        "])\n",
        "\n",
        "# Fitting the pipe on the training data\n",
        "SVCpipe.fit(Xt, yt)\n",
        "\n",
        "# Finding cross-validated accuracy\n",
        "print(np.mean(cross_val_score(SVCpipe, X, y, cv=5, scoring='accuracy')))\n",
        "\n",
        "# Generating predictions on the validation set\n",
        "y_pred = SVCpipe.predict(Xv)\n",
        "\n",
        "# specifying class order for rows/columns\n",
        "labels = [False, True]\n",
        "\n",
        "# Designing a confusion matrix\n",
        "cm = confusion_matrix(yv, y_pred, labels=labels)\n",
        "cm_df = pd.DataFrame(cm, index=[f\"Actual {lbl}\" for lbl in labels], # appending row labels\n",
        "                    columns=[f\"Predicted {lbl}\" for lbl in labels]) # appending column labels\n",
        "print(cm_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "cv score: 0.7781945267887791"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# For Logistic Regression\n",
        "logit = Pipeline([\n",
        "    ('preprocess', ct),\n",
        "    ('model', LogisticRegression())\n",
        "])\n",
        "\n",
        "# Fitting the pipe on the training data\n",
        "logit.fit(Xt, yt)\n",
        "\n",
        "# Finding cross-validated accuracy\n",
        "print(np.mean(cross_val_score(logit, X, y, cv=5, scoring='accuracy')))\n",
        "\n",
        "# Generating predictions on the validation set\n",
        "y_pred = logit.predict(Xv)\n",
        "\n",
        "# specifying class order for rows/columns\n",
        "labels = [False, True]\n",
        "\n",
        "# Designing a confusion matrix\n",
        "cm = confusion_matrix(yv, y_pred, labels=labels)\n",
        "cm_df = pd.DataFrame(cm, index=[f\"Actual {lbl}\" for lbl in labels], # appending row labels\n",
        "                    columns=[f\"Predicted {lbl}\" for lbl in labels]) # appending column labels\n",
        "print(cm_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "cv score: 0.7864085041761579\n",
        "\n",
        "Sativa vs. Not Sativa"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# For SVC\n",
        "tmp = base.copy()\n",
        "tmp['sativa'] = (tmp['Type'] == 'sativa')\n",
        "\n",
        "X = tmp.drop(columns=['Type', 'sativa'])\n",
        "y = tmp['sativa']\n",
        "\n",
        "Xt, Xv, yt, yv = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Getting numeric and categorical variables\n",
        "num = X.select_dtypes(include=['number']).columns\n",
        "cat = X.select_dtypes(exclude=['number']).columns\n",
        "\n",
        "# Defining columnTransformer for preprocessing steps\n",
        "ct = ColumnTransformer(\n",
        "    transformers=[\n",
        "    ('num', StandardScaler(), num),\n",
        "    ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), cat)\n",
        "])\n",
        "\n",
        "SVCpipe = Pipeline([\n",
        "    ('preprocess', ct),\n",
        "    ('model', SVC())\n",
        "])\n",
        "\n",
        "# Fitting the pipe on the training data\n",
        "SVCpipe.fit(Xt, yt)\n",
        "\n",
        "# Finding cross-validated accuracy\n",
        "print(np.mean(cross_val_score(SVCpipe, X, y, cv=5, scoring='accuracy')))\n",
        "\n",
        "# Generating predictions on the validation set\n",
        "y_pred = SVCpipe.predict(Xv)\n",
        "\n",
        "# specifying class order for rows/columns\n",
        "labels = [False, True]\n",
        "\n",
        "# Designing a confusion matrix\n",
        "cm = confusion_matrix(yv, y_pred, labels=labels)\n",
        "cm_df = pd.DataFrame(cm, index=[f\"Actual {lbl}\" for lbl in labels], # appending row labels\n",
        "                    columns=[f\"Predicted {lbl}\" for lbl in labels]) # appending column labels\n",
        "print(cm_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "CV scores: 0.8188057124431823"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# For Logistic Regression\n",
        "logit = Pipeline([\n",
        "    ('preprocess', ct),\n",
        "    ('model', LogisticRegression())\n",
        "])\n",
        "\n",
        "# Fitting the pipe on the training data\n",
        "logit.fit(Xt, yt)\n",
        "\n",
        "# Finding cross-validated accuracy\n",
        "print(np.mean(cross_val_score(logit, X, y, cv=5, scoring='accuracy')))\n",
        "\n",
        "# Generating predictions on the validation set\n",
        "y_pred = logit.predict(Xv)\n",
        "\n",
        "# specifying class order for rows/columns\n",
        "labels = [False, True]\n",
        "\n",
        "# Designing a confusion matrix\n",
        "cm = confusion_matrix(yv, y_pred, labels=labels)\n",
        "cm_df = pd.DataFrame(cm, index=[f\"Actual {lbl}\" for lbl in labels], # appending row labels\n",
        "                    columns=[f\"Predicted {lbl}\" for lbl in labels]) # appending column labels\n",
        "print(cm_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "CV scores: 0.825653987372713\n",
        "\n",
        "\n",
        "Hybrid vs. Not Hybrid"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# For SVC\n",
        "tmp = base.copy()\n",
        "tmp['hybrid'] = (tmp['Type'] == 'hybrid')\n",
        "\n",
        "X = tmp.drop(columns=['Type', 'hybrid'])\n",
        "y = tmp['hybrid']\n",
        "Xt, Xv, yt, yv = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Getting numeric and categorical variables\n",
        "num = X.select_dtypes(include=['number']).columns\n",
        "cat = X.select_dtypes(exclude=['number']).columns\n",
        "\n",
        "# Defining columnTransformer for preprocessing steps\n",
        "ct = ColumnTransformer(\n",
        "    transformers=[\n",
        "    ('num', StandardScaler(), num),\n",
        "    ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), cat)\n",
        "])\n",
        "\n",
        "SVCpipe = Pipeline([\n",
        "    ('preprocess', ct),\n",
        "    ('model', SVC())\n",
        "])\n",
        "\n",
        "# Fitting the pipe on the training data\n",
        "SVCpipe.fit(Xt, yt)\n",
        "\n",
        "# Finding cross-validated accuracy\n",
        "print(np.mean(cross_val_score(SVCpipe, X, y, cv=5, scoring='accuracy')))\n",
        "\n",
        "# Generating predictions on the validation set\n",
        "y_pred = SVCpipe.predict(Xv)\n",
        "\n",
        "# specifying class order for rows/columns\n",
        "labels = [False, True]\n",
        "\n",
        "# Designing a confusion matrix\n",
        "cm = confusion_matrix(yv, y_pred, labels=labels)\n",
        "cm_df = pd.DataFrame(cm, index=[f\"Actual {lbl}\" for lbl in labels], # appending row labels\n",
        "                    columns=[f\"Predicted {lbl}\" for lbl in labels]) # appending column labels\n",
        "print(cm_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "cv scores: 0.6147834950749421"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# For Logistic Regression\n",
        "logit = Pipeline([\n",
        "    ('preprocess', ct),\n",
        "    ('model', LogisticRegression())\n",
        "])\n",
        "\n",
        "# Fitting the pipe on the training data\n",
        "logit.fit(Xt, yt)\n",
        "\n",
        "# Finding cross-validated accuracy\n",
        "print(np.mean(cross_val_score(logit, X, y, cv=5, scoring='accuracy')))\n",
        "\n",
        "# Generating predictions on the validation set\n",
        "y_pred = logit.predict(Xv)\n",
        "\n",
        "# specifying class order for rows/columns\n",
        "labels = [False, True]\n",
        "\n",
        "# Designing a confusion matrix\n",
        "cm = confusion_matrix(yv, y_pred, labels=labels)\n",
        "cm_df = pd.DataFrame(cm, index=[f\"Actual {lbl}\" for lbl in labels], # appending row labels\n",
        "                    columns=[f\"Predicted {lbl}\" for lbl in labels]) # appending column labels\n",
        "print(cm_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "CV scores: 0.6193538656764543\n",
        "\n",
        "Q2\n",
        "Which of the six models did the best job distinguishing the target category from the rest? Which did the worst? Does this make intuitive sense?\n",
        "\n",
        "When assessing which of the 6 models did the best job distinguishing the target category from the rest, the Sativa vs. rest Logistic Regression (cv=.826), followed closely by Sativa SVC. The worst was Hybrid vs. rest (SVC = 0.615, logit = 0.619). Overall, this makes sense, as Sativa is the most distinct from the other two types, while Hybrid overlaps both Indica and Sativa, making it the hardest one vs rest separation.\n",
        "\n",
        "Q3\n",
        "Fit and report metrics for OvO versions of the models. That is, for each of the two model types, create three models:\n",
        "\n",
        "Indica vs. Sativa"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# For Logistic Regression\n",
        "ovo = df.dropna()\n",
        "ovo = ovo.copy()\n",
        "ovo = ovo[ovo['Type'].isin(['indica', 'sativa'])]\n",
        "\n",
        "ovo['indica_v_sativa'] = ovo['Type'] == 'indica'\n",
        "\n",
        "X = ovo.drop(columns=['Type', 'indica_v_sativa'])\n",
        "y = ovo['indica_v_sativa']\n",
        "\n",
        "Xt, Xv, yt, yv = train_test_split(X,y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Creating the Logistic Regression Pipeline\n",
        "\n",
        "# Getting numeric and categorical variables\n",
        "num = X.select_dtypes(include=['number']).columns\n",
        "cat = X.select_dtypes(exclude=['number']).columns\n",
        "\n",
        "ct = ColumnTransformer([\n",
        "      ('num', StandardScaler(), num),\n",
        "      ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), cat)\n",
        "])\n",
        "\n",
        "logit = Pipeline([\n",
        "    ('preprocess', ct),\n",
        "    ('model', LogisticRegression())\n",
        "])\n",
        "\n",
        "# Fitting the OvO model on the training dataset\n",
        "logit.fit(Xt, yt)\n",
        "\n",
        "# Finding cross-validated accuracy\n",
        "print(np.mean(cross_val_score(logit, X, y, cv=5, scoring='accuracy')))\n",
        "\n",
        "# Generating predictions on the validation set\n",
        "y_pred = logit.predict(Xv)\n",
        "\n",
        "# specifying class order for rows/columns\n",
        "labels = [False, True]\n",
        "\n",
        "# Designing a confusion matrix\n",
        "cm = confusion_matrix(yv, y_pred, labels=labels)\n",
        "cm_df = pd.DataFrame(cm, index=[f\"Actual {lbl}\" for lbl in labels], # appending row labels\n",
        "                    columns=[f\"Predicted {lbl}\" for lbl in labels]) # appending column labels\n",
        "print(cm_df)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cross validated accuracy score: 0.8539423456627617\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# For Logistic Regression\n",
        "ovo = df.dropna()\n",
        "ovo = ovo.copy()\n",
        "ovo = ovo[ovo['Type'].isin(['indica', 'sativa'])]\n",
        "\n",
        "ovo['indica_v_sativa'] = ovo['Type'] == 'indica'\n",
        "\n",
        "X = ovo.drop(columns=['Type', 'indica_v_sativa'])\n",
        "y = ovo['indica_v_sativa']\n",
        "\n",
        "Xt, Xv, yt, yv = train_test_split(X,y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Creating the SVM Pipeline\n",
        "\n",
        "svm = Pipeline([\n",
        "    ('preprocess', ct),\n",
        "    ('model', SVC())\n",
        "])\n",
        "\n",
        "# Fitting the OvO model on the training dataset\n",
        "svm.fit(Xt, yt)\n",
        "\n",
        "# Finding cross-validated accuracy\n",
        "print(np.mean(cross_val_score(svm, X, y, cv=5, scoring='accuracy')))\n",
        "\n",
        "# Generating predictions on the validation set\n",
        "y_pred = svm.predict(Xv)\n",
        "\n",
        "# specifying class order for rows/columns\n",
        "labels = [False, True]\n",
        "\n",
        "# Designing a confusion matrix\n",
        "cm = confusion_matrix(yv, y_pred, labels=labels)\n",
        "cm_df = pd.DataFrame(cm, index=[f\"Actual {lbl}\" for lbl in labels], # appending row labels\n",
        "                    columns=[f\"Predicted {lbl}\" for lbl in labels]) # appending column labels\n",
        "print(cm_df)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cross validation accuracy score: 0.8408757843008206\n",
        "\n",
        "Indica vs. Hybrid"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# For Logistic Regression\n",
        "ovo = df.dropna()\n",
        "ovo = ovo.copy()\n",
        "ovo = ovo[ovo['Type'].isin(['indica', 'hybrid'])]\n",
        "\n",
        "ovo['indica_v_hybrid'] = ovo['Type'] == 'indica'\n",
        "\n",
        "X = ovo.drop(columns=['Type', 'indica_v_hybrid'])\n",
        "y = ovo['indica_v_hybrid']\n",
        "\n",
        "Xt, Xv, yt, yv = train_test_split(X,y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Creating the Logistic Regression Pipeline\n",
        "\n",
        "# Getting numeric and categorical variables\n",
        "num = X.select_dtypes(include=['number']).columns\n",
        "cat = X.select_dtypes(exclude=['number']).columns\n",
        "\n",
        "ct = ColumnTransformer([\n",
        "      ('num', StandardScaler(), num),\n",
        "      ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), cat)\n",
        "])\n",
        "\n",
        "logit = Pipeline([\n",
        "    ('preprocess', ct),\n",
        "    ('model', LogisticRegression())\n",
        "])\n",
        "\n",
        "# Fitting the OvO model on the training dataset\n",
        "logit.fit(Xt, yt)\n",
        "\n",
        "# Finding cross-validated accuracy\n",
        "print(np.mean(cross_val_score(logit, X, y, cv=5, scoring='accuracy')))\n",
        "\n",
        "# Generating predictions on the validation set\n",
        "y_pred = logit.predict(Xv)\n",
        "\n",
        "# specifying class order for rows/columns\n",
        "labels = [False, True]\n",
        "\n",
        "# Designing a confusion matrix\n",
        "cm = confusion_matrix(yv, y_pred, labels=labels)\n",
        "cm_df = pd.DataFrame(cm, index=[f\"Actual {lbl}\" for lbl in labels], # appending row labels\n",
        "                    columns=[f\"Predicted {lbl}\" for lbl in labels]) # appending column labels\n",
        "print(cm_df)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cross validated accuracy score: 0.7401882101155068\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "ovo = df.dropna()\n",
        "ovo = ovo.copy()\n",
        "ovo = ovo[ovo['Type'].isin(['indica', 'hybrid'])]\n",
        "\n",
        "ovo['indica_v_hybrid'] = ovo['Type'] == 'indica'\n",
        "\n",
        "X = ovo.drop(columns=['Type', 'indica_v_hybrid'])\n",
        "y = ovo['indica_v_hybrid']\n",
        "\n",
        "Xt, Xv, yt, yv = train_test_split(X,y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Creating the SVM Pipeline\n",
        "\n",
        "svm = Pipeline([\n",
        "    ('preprocess', ct),\n",
        "    ('model', SVC())\n",
        "])\n",
        "\n",
        "# Fitting the OvO model on the training dataset\n",
        "svm.fit(Xt, yt)\n",
        "\n",
        "# Finding cross-validated accuracy\n",
        "print(np.mean(cross_val_score(svm, X, y, cv=5, scoring='accuracy')))\n",
        "\n",
        "# Generating predictions on the validation set\n",
        "y_pred = svm.predict(Xv)\n",
        "\n",
        "# specifying class order for rows/columns\n",
        "labels = [False, True]\n",
        "\n",
        "# Designing a confusion matrix\n",
        "cm = confusion_matrix(yv, y_pred, labels=labels)\n",
        "cm_df = pd.DataFrame(cm, index=[f\"Actual {lbl}\" for lbl in labels], # appending row labels\n",
        "                    columns=[f\"Predicted {lbl}\" for lbl in labels]) # appending column labels\n",
        "print(cm_df)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cross validation accuracy score: 0.7317801277814496\n",
        "\n",
        "Hybrid vs. Sativa"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# For Logistic Regression\n",
        "ovo = df.dropna()\n",
        "ovo = ovo.copy()\n",
        "ovo = ovo[ovo['Type'].isin(['hybrid', 'sativa'])]\n",
        "\n",
        "ovo['hybrid_v_sativa'] = ovo['Type'] == 'hybrid'\n",
        "\n",
        "X = ovo.drop(columns=['Type', 'hybrid_v_sativa'])\n",
        "y = ovo['hybrid_v_sativa']\n",
        "\n",
        "Xt, Xv, yt, yv = train_test_split(X,y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Creating the Logistic Regression Pipeline\n",
        "\n",
        "# Getting numeric and categorical variables\n",
        "num = X.select_dtypes(include=['number']).columns\n",
        "cat = X.select_dtypes(exclude=['number']).columns\n",
        "\n",
        "ct = ColumnTransformer([\n",
        "      ('num', StandardScaler(), num),\n",
        "      ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), cat)\n",
        "])\n",
        "\n",
        "logit = Pipeline([\n",
        "    ('preprocess', ct),\n",
        "    ('model', LogisticRegression())\n",
        "])\n",
        "\n",
        "# Fitting the OvO model on the training dataset\n",
        "logit.fit(Xt, yt)\n",
        "\n",
        "# Finding cross-validated accuracy\n",
        "print(np.mean(cross_val_score(logit, X, y, cv=5, scoring='accuracy')))\n",
        "\n",
        "# Generating predictions on the validation set\n",
        "y_pred = logit.predict(Xv)\n",
        "\n",
        "# specifying class order for rows/columns\n",
        "labels = [False, True]\n",
        "\n",
        "# Designing a confusion matrix\n",
        "cm = confusion_matrix(yv, y_pred, labels=labels)\n",
        "cm_df = pd.DataFrame(cm, index=[f\"Actual {lbl}\" for lbl in labels], # appending row labels\n",
        "                    columns=[f\"Predicted {lbl}\" for lbl in labels]) # appending column labels\n",
        "print(cm_df)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cross validated accuracy score: 0.7506482723382513\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "ovo = df.dropna()\n",
        "ovo = ovo.copy()\n",
        "ovo = ovo[ovo['Type'].isin(['hybrid', 'sativa'])]\n",
        "\n",
        "ovo['hybrid_v_sativa'] = ovo['Type'] == 'hybrid'\n",
        "\n",
        "X = ovo.drop(columns=['Type', 'hybrid_v_sativa'])\n",
        "y = ovo['hybrid_v_sativa']\n",
        "\n",
        "Xt, Xv, yt, yv = train_test_split(X,y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Creating the SVM Pipeline\n",
        "\n",
        "svm = Pipeline([\n",
        "    ('preprocess', ct),\n",
        "    ('model', SVC())\n",
        "])\n",
        "\n",
        "# Fitting the OvO model on the training dataset\n",
        "svm.fit(Xt, yt)\n",
        "\n",
        "# Finding cross-validated accuracy\n",
        "print(np.mean(cross_val_score(svm, X, y, cv=5, scoring='accuracy')))\n",
        "\n",
        "# Generating predictions on the validation set\n",
        "y_pred = svm.predict(Xv)\n",
        "\n",
        "# specifying class order for rows/columns\n",
        "labels = [False, True]\n",
        "\n",
        "# Designing a confusion matrix\n",
        "cm = confusion_matrix(yv, y_pred, labels=labels)\n",
        "cm_df = pd.DataFrame(cm, index=[f\"Actual {lbl}\" for lbl in labels], # appending row labels\n",
        "                    columns=[f\"Predicted {lbl}\" for lbl in labels]) # appending column labels\n",
        "print(cm_df)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cross validation accuracy score: 0.7336739690447297\n",
        "\n",
        "Q4\n",
        "Which of the six models did the best job distinguishing at differentiating the two groups? Which did the worst? Does this make intuitive sense?\n",
        "\n",
        "The sativa v. indica logistic regression model performed the best in terms of cross validated score, with an accuracy of ~0.85. The model that performed the worst was the indica v. hybrid SVM model, with a cross validated accuracy score of ~0.73. Intuitively, this makes sense, as sativa versus indica strains separate more cleanly, but hybrids overlap the types, making a harder boundary for the SVM model.\n",
        "\n",
        "\n",
        "Q5\n",
        "Suppose you had simply input the full data, with three classes, into the LogisticRegression function. Would this have automatically taken an “OvO” approach or an “OvR” approach?\n",
        "\n",
        "For logistic regression, by default the (multi_class='auto' with a multinomial-capable solver like lbfgs), it fits a multinomial model, not OvO. If you pick a solver that can't do multinomial, it falls back to one-versus-rest.\n",
        "\n",
        "\n",
        "What about for SVC?\n",
        "For multi-class inputs, it always uses one-vs-one.\n",
        "\n",
        "\n",
        "Note: You do not actually have to run code here - you only need to look at sklearn’s documentation to see how these functions handle multiclass input."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/opt/anaconda3/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}