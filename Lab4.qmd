---
title: GSBS544 Lab 4
author: Sahil Bains
format:
  html:
    embed-resources: true
    code-fold: true
echo: true
---
Github Link: https://github.com/sbains2/GSBS544

### DEFINING HELPER FUNCTIONS

```{python}
import requests
from bs4 import BeautifulSoup
```
```{python}
# Preparing searchable query string
def search_query(s):
    """
    Converts query string into lowercase, keeps all numbers, removes spaces
    """

    # keeping lowercase, numbers, and spaces. Removing whitespace
    s = s.lower()
    s = re.sub(r'[^a-z0-9\s]', ' ', s)
    s = re.sub(r'\s+', ' ', s).strip()
    return s
```

```{python}
# Normalizing API JSON into a clean table

def flatten(item):
    """
    Extracts a simple ingredients list from Tasty JSON
    """
    # Retrieving sections
    sections = item.get('sections') or []
    # Initializing container for character strings
    parts = []
    # Iterating through section blocks
    for i in sections:
        # Iterating through the components in the section block
        for component in i.get('components', []):
            # Get the ingredient text if available
            raw_string = component.get('raw_text')
            if raw_string:
                # Only appending non-empty values
                parts.append(raw_string)
    # Join with semi-colons or return None
    return "; ".join(parts) if parts else None


def to_row(item):
    """
    Picking useful fields from Tasty recipe item
    """
    ratings = item.get('user_ratings') or {}
    # Used to form tasty.co URL
    slug = item.get('slug')
    return {
        'tasty_id': item.get('id'),
        'title': item.get('name'),
        'description': item.get('description'),
        'num_servings': item.get('num_servings'),
        'total_time_minutes': item.get('total_time_minutes'),
        'video_url': item.get('video_url'),
        'recipe_url': f'https://tasty.co/recipe/{slug}' if slug else None, # canonical recipe page
        'ingredients': flatten(item), # flattened ingredient string
        'nutrition': item.get('nutrition')
    }

```

1. Data from unstructured websites
This website contains many weekly meal plans. Choose one that seems delicious to you. Scrape the weekly meal plan into a table with the following columns:

1. Day of the Week
2. Name of Recipe
3. Link to Recipe
4. Price of Recipe

```{python}
# Importing libraries
import pandas as pd
import numpy as np
import re
import bs4

def scrape_week_plan(plan=202):
    """
    Scraping a weekly meal plan from 'tastesbetterfromscratch' plan number 210
    Returns DataFrame with days, recipes, links, prices
    """

    # Setting up soup
    url = f"https://tastesbetterfromscratch.com/meal-plan-{plan}/"
    headers = {"User-Agent": "Chrome (Sahil)"}
    response = requests.get(url, headers=headers)
    soup = BeautifulSoup(response.content, "html.parser")


    # Finding all p's
    ps = soup.find_all('p')

    # Creating empty lists for each column
    days = []
    recipes = []
    links = []
    prices = []

    # Looping through p tags and filtering for those that start with a day pattern
    for p in ps:
        text = p.get_text()
        # Only keeping lines that have ':' and '$'
        if ':' in text and '$' in text:
            # Splitting at the first colon: "Monday", rest of info...
            day, info = text.split(':', 1)
            day = day.strip()
            # Recipe is in the part before the first '$'
            recipe = info.split('$')[0].strip()
            # Price is in the first token after '$'
            price = '$' + info.split('$')[-1].split()[0].strip()
            # Extracting link
            link_tag = p.find('a')
            link = link_tag['href'] if link_tag else None
            # Appending the results
            days.append(day)
            recipes.append(recipe)
            links.append(link)
            prices.append(price)
    # Framing in df
    plan = pd.DataFrame({
        'day': days,
        'recipe_name': recipes,
        'source_link': links,
        'price': prices
    })
    return plan

print(scrape_week_plan(202))
```

2. Data from an API
Using the Tasty API from the practice activity, search for recipes that match the “Monday” recipe in your meal plan. Compile a table of all these recipes.

(Warning - your free Tasty API account only allows 500 queries per month. You should not need more than this, but if you do end up with a recipe that has a large number of matches, you only need to scrape the first 100 of them.)

Hint: You may need to use some text editing tricks to convert the recipe name into a searchable string. Not all recipe names will find a match on the Tasty API; that’s okay and you can leave these blank.


```{python}
api = "a1f843d8f0msh79f993122eb68ecp1b45bajsn88d40dbd6eeb"

# Calling Tasty API
def fetch_tasty(name, api, max_results = 100, batch = 20):
    
    url = "https://tasty.p.rapidapi.com/recipes/list"
    headers = {
	"x-rapidapi-key": api,
	"x-rapidapi-host": "tasty.p.rapidapi.com"
    }
    # normalize search text using helper 
    query = search_query(name)
    # Initializing list to accumulate all items
    results = []
    # Initializing pagination offset
    offset = 0
    # Looping until reaching the cap
    while offset < max_results:
        # Choosing the batch size within the cap
        size = min(batch, max_results - offset)
        # Setting query parameters for the page
        parameters = {'q': query, "from": offset, 'size': str(size)}
        req = requests.get(url, headers=headers, params=parameters)
        data = req.json()
        # Pulling items list and only keeping dicts with a name
        items = [i for i in data.get('results', []) if isinstance(i, dict) and i.get('name')]
        if not items:
            break
        results.extend(items)
        # Advancing the pagination
        offset += size
    return results

```


```{python}
# Defining plan
plan = scrape_week_plan(202)

# Pulling the Monday recipe from the 'plan' dataframe
monday = (
    plan.loc[plan['day'].str.contains('mon', case=False, na=False)]
    .iloc[0]
)

# normalizing search term
term = search_query(monday['recipe_name'])

url = "https://tasty.p.rapidapi.com/recipes/list"

headers = {
	"x-rapidapi-key": api,
	"x-rapidapi-host": "tasty.p.rapidapi.com"
    }

all_results = []
# Defining 5 pages
for i in range(0, 100, 20):
    # Getting page 'i'
    res = requests.get(url, headers=headers, params={'from': i, 'size': 20, 'q': term})
    # List of items
    items = res.json().get('results', []) if res.ok else []
    # Stop once no results
    if not items:
        break
    all_results += items

# Fallback - trying the first word only
if not all_results and ' ' in term:
    # will recognize 'chicken' in 'chicken divan'
    term = term.split()[0]
    for i in range(0, 100, 20):
        res = requests.get(url, headers=headers, params={'from': i, 'size': 20, 'q': term})
        items = res.json().get('results', []) if res.ok else []
        if not items:
            break
        all_results += items

# Keeping only recipes
recipes = [i for i in all_results if str(i.get('canonical_id', '')).startswith('recipe:')]

# Building monday list
mondays_list = pd.DataFrame({
    'day': [monday['day']] * len(recipes),
    'recipe_name': [monday['recipe_name']] * len(recipes),
    'source_link': [monday['source_link']] * len(recipes),
    'price': [monday['price']] * len(recipes),
    'tasty_title': [i.get('name') for i in recipes],
    'tasty_url': [f"https://tasty.co/recipe/{i.get('slug')}" if i.get('slug') else None for i in recipes],
})

print(mondays_list)

```

3. Automate it
Write a function called get_mealplan_data that performs 2 and 3 above automatically. That is, your function should:

1. Take as input a number 100-210, representing which weekly meal plan you are referencing.

2. Scrape the meal plan from the meal planning site.

3. Query the Tasty API for recipes matching each of the ones in the chosen weekly meal plan.

4. Output a single dataset, which contains all the information from the above

Hint: You may have an easier time if you write two smaller functions, get_weekly_plan and match_recipe, and then you use them inside your main function.

Run the following code, which should work if your function is complete:

df = get_mealplan_data(202)


```{python}
def get_mealplan_data(plan, api):
    plan = scrape_week_plan(plan)

    # List to hold data from all days
    every_match = []

    # Iterate rows for each recipe and fetch matches
    for i, row in plan.iterrows():
        day = row['day']
        name = row['recipe_name']
        link = row['source_link']
        price = row['price']

        # Search query
        query = search_query(name)
        items = fetch_tasty(name, api=api,max_results=100, batch=20)
        rows = [to_row(i) for i in items]
        matches = pd.DataFrame(rows)

        # Combining data
        if len(matches) > 0:
            matches['days'] = day
            matches['recipes'] = name
            matches['links'] = link
            matches['prices'] = price
            matches['search_query'] = query
            every_match.append(matches)
        else:
            # Placeholder if row isn't found
            place = pd.DataFrame({
                'days': [day],
                'recipe_names': [name],
                'links': [link],
                'prices': [price],
                'search_q': [query]
            })
            every_match.append(place)
    return pd.concat(every_match, ignore_index=True)


```


4. Add a column with fuzzy matching
Add a column to your df dataset indicating whether the recipe in that row is vegetarian or not.

You may assume, for our purposes, that all recipes containing meat will have the name of a common meat in the recipe title. (Of course, that is not universally true - but we’ll assume it is for now.)

```{python}
def add_veg_col(df):
    """
    Adds a column is_veg based on the recipe title.
    Assumes that recipes containing the meat words are non-vegetarian
    """
    # Defining meats
    meats = ['chicken', 'pork', 'veal', 'lamb', 'turkey', 'salmon', 'tuna',
            'bacon', 'sausage', 'fish', 'ham', 'shrimp', 'beef']
    # Normalizing title string
    col = df['title'].astype(str).str.lower()
    # True if none of the meats match up
    df['is_vegetarian'] = ~col.str.contains('|'.join(meats))
    return df
```

```{python}
df = get_mealplan_data(202, api=api)
df = add_veg_col(df)
print(df.head())
```

5. Analyze
Make a visualization that tells a story about nutrition information (available in the Tasty API results) across the week for Mealplan 202. Your visualization should also indicate which meals are vegetarian.

```{python}
# Creating a function to tidy the dataframe
def preparing_df(df):
    """
    Extracting key columns, ordering the days, and melting to long format for easier plotting
    """
    if 'is_vegetarian' not in df.columns:
        df = add_veg_col(df)

    #1 Extracting nutrition
    for key in ['calories', 'protein', 'fat', 'carbohydrates']:
        df[key] = df['nutrition'].apply(
            lambda i: i.get(key) if isinstance(i, dict) else None
        )
    
    #2: Ordering the days so that the x-axis is readable
    days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']
    if df['days'].isin(days).any():
        df['days'] = pd.Categorical(df['days'], categories=days, ordered=True)
    
    #3: Aggregating is_vegetarian x days mean
    aggregate = (
        df.groupby(['days', 'is_vegetarian'], as_index=False, observed=True)
        .agg({'calories': 'mean', 'protein': 'mean', 'fat': 'mean', 'carbohydrates': 'mean'})
    )

    #4: melting from wide to long to facet across nutrients
    formatted = aggregate.melt(
        id_vars=['days', 'is_vegetarian'],
        value_vars=['calories', 'protein', 'fat', 'carbohydrates'],
        var_name='nutrient',
        value_name='value',
    ).dropna(subset=['value'])

    # relabeling
    formatted['nutrient'] = formatted['nutrient'].map({
        'calories': "Calories",
        'protein': "Protein (g)",
        'fat': 'Fat (g)',
        'carbohydrates': 'Carbs (g)'
    })
    return formatted
```

```{python}

from plotnine import *
import numpy as np

def plot_meal_times(df):
    """
    Visualizing total_time_minutes by day
    color-coded by vegetarian meal
    """
    formatted = preparing_df(df)

    # Creating the plot
    p = (
    ggplot(formatted, aes(x='days', y='value', fill='is_vegetarian'))
    + geom_col(position='dodge')
    + facet_wrap('~nutrient', scales='free_y', ncol=2)
    + labs(
        title='Meal plan 202 - Nutrient by day (Vegetarian vs. Non-Veg)',
        x='Day of the week',
        y='Average per recipe',
        fill='Vegetarian?')

    + theme(
        axis_text_x=element_text(rotation=45, ha='right'),
        legend_position='top'
    )
    )
    return p
```

```{python}
plot = plot_meal_times(df)
plot
```

Takeaway:
Vegetarian meals in plan 202 run higher in carbs (especially Wednesday and Thursday) and often higher in
calories on those days, Tuesday is approximately even on calories between non-veg and vegetarian meals. Non vegetarian dishes are higher in protein on Tuesday and Thursday, but wednesday shows vegetarian dishes having a higher protein count. Some gaps in the plot are that friday shows only non-vegetarian bars (because there is no vegetarian match returned), which is why there isn't a side-by-side comparison for that day.