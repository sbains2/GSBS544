---
title: GSBS544 Lab 7
author: Sahil Bains
format:
  html:
    embed-resources: true
    code-fold: true
echo: true
---
Github Link: https://github.com/sbains2/GSBS544

The Data
In this lab, we will use medical data to predict the likelihood of a person experiencing an exercise-induced heart attack.

Our dataset consists of clinical data from patients who entered the hospital complaining of chest pain (“angina”) during exercise. The information collected includes:

age : Age of the patient

sex : Sex of the patient

cp : Chest Pain type

Value 0: asymptomatic
Value 1: typical angina
Value 2: atypical angina
Value 3: non-anginal pain

trtbps : resting blood pressure (in mm Hg)

chol : cholesterol in mg/dl fetched via BMI sensor

restecg : resting electrocardiographic results

Value 0: normal
Value 1: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV)
Value 2: showing probable or definite left ventricular hypertrophy by Estes’ criteria
thalach : maximum heart rate achieved during exercise

output : the doctor’s diagnosis of whether the patient is at risk for a heart attack

0 = not at risk of heart attack
1 = at risk of heart attack
Although it is not a formal question on this assignment, you should begin by reading in the dataset and briefly exploring and summarizing the data, and by adjusting any variables that need cleaning.



```{python}
import pandas as pd
import numpy as np
from sklearn.pipeline import Pipeline
from sklearn.compose import make_column_selector, ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import r2_score, mean_squared_error, roc_auc_score, confusion_matrix, roc_curve
import math
from plotnine import *
```


```{python}
ha = pd.read_csv("ha.csv")
print(ha.describe())
print(ha.info())
```

Part One: Fitting Models
This section asks you to create a final best model for each of the model types studied this week. For each, you should:

Find the best model based on ROC AUC for predicting the target variable.

Report the (cross-validated!) ROC AUC metric.

Fit the final model.

Output a confusion matrix; that is, the counts of how many observations fell into each predicted class for each true class.

(Where applicable) Interpret the coefficients and/or estimates produced by the model fit.

You should certainly try multiple model pipelines to find the best model. You do not need to include the output for every attempted model, but you should describe all of the models explored. You should include any hyperparameter tuning steps in your writeup as well.

Q1: KNN
Q2: Logistic Regression
Q3: Decision Tree
Q4: Interpretation, Which predictors were most important to predicting heart attack risk?

Q5: ROC Curve
Plot the ROC Curve for your three models above.

## Part 1
Q1:
```{python}

# Defining target and explanatory variables
X = ha.drop(columns='output')
y = ha['output']

# Defining numerical and categorical features
numeric = ['age', 'trtbps', 'chol', 'thalach']
categorical = ['sex', 'cp', 'restecg']

# Building the preprocessing transformer: scales the numerics, one-hot encodes the categoricals
ct = ColumnTransformer(
  [
    ('numeric', StandardScaler(), numeric),
    ('categorical', OneHotEncoder(handle_unknown='ignore'), categorical),
  ]
)

# Splitting the data into train and testing - holding out 20% for validation so that all the models get the same split
Xt, Xv, yt, yv = train_test_split(X, y, test_size=0.2, random_state=42)

# Developing knn pipeline
knn_pipe = Pipeline([
  ('prep', ct),
  ('knn', KNeighborsClassifier())
])

# Iterating through different values of k
knn_params = {
  'knn__n_neighbors': [9,11,13],
  'knn__weights': ['uniform', 'distance']
}

# Initializing grid search
knn_search = GridSearchCV(
  knn_pipe,
  param_grid=knn_params,
  cv=5,
  scoring='roc_auc'
)
knn_search.fit(Xt, yt)

# Finding the best estimators and parameters
best_knn = knn_search.best_estimator_
print('best params:', knn_search.best_params_)
print('CV ROC AUC', round(knn_search.best_score_, 3))

# Evaluating on the held-out validation split to capture testing metrics
y_val_proba = best_knn.predict_proba(Xv)[:, 1]
y_val_pred = best_knn.predict(Xv)
cm_val = pd.DataFrame(
  confusion_matrix(yv, y_val_pred),
  index=['True 0', 'True 1'],
  columns=['Pred 0', 'Pred 1']
)
print('Validation ROC AUC', roc_auc_score(yv, y_val_proba))
print(cm_val)
tn, fp, fn, tp = confusion_matrix(yv, y_val_pred).ravel()
print(f'Validation TPR: {tp / (tp + fn)}')
print(f'Validation Precision: {tp / (tp + fp)}')
print(f'Validation TNR: {tn / (tn + fp)}')
# The testing kNN metrics are: TPR ≈ 0.414, Precision ≈ 0.571, TNR ≈ 0.654 on this split.

```

Q2:

```{python}

# Developing logit pipeline
logit_pipe = Pipeline([
  ('prep', ct),
  ('logit', LogisticRegression())
])

# Creating hyperparameters for the logistic regression model
logit_params = {
  'logit__C': [0.001, 0.01, 0.1, 1, 10, 100],
}

# Initializing grid search
logit_search = GridSearchCV(
  logit_pipe,
  param_grid=logit_params,
  cv=5,
  scoring='roc_auc'
)
logit_search.fit(Xt, yt)

# Finding the best estimators and parameters
best_logit = logit_search.best_estimator_
print('best params:', logit_search.best_params_)
print('CV ROC AUC', round(logit_search.best_score_, 3))

# Evaluating on the held-out validation split to capture testing metrics
y_val_proba = best_logit.predict_proba(Xv)[:, 1]
y_val_pred = best_logit.predict(Xv)
cm_val = pd.DataFrame(
  confusion_matrix(yv, y_val_pred),
  index=['True 0', 'True 1'],
  columns=['Pred 0', 'Pred 1']
)
print('Validation ROC AUC', roc_auc_score(yv, y_val_proba))
print(cm_val)
tn, fp, fn, tp = confusion_matrix(yv, y_val_pred).ravel()
print(f'Validation TPR: {tp / (tp + fn)}')
print(f'Validation Precision: {tp / (tp + fp)}')
print(f'Validation TNR: {tn / (tn + fp)}')
# The testing metrics for logistic regression is TPR ≈ 0.586, Precision ≈ 0.680, TNR ≈ 0.692.

```

Q3:

```{python}

# Developing dt pipeline
tree_pipe = Pipeline([
  ('prep', ct),
  ('tree', DecisionTreeClassifier(random_state=42))
])

# Creating hyperparameters for the decision tree model
tree_params = {
  'tree__max_depth': [2,4,6,8],
  'tree__min_samples_leaf': [1,3,5]
}

# Initializing grid search
tree_search = GridSearchCV(
  tree_pipe,
  param_grid=tree_params,
  cv=5,
  scoring='roc_auc'
)
tree_search.fit(Xt, yt)

# Finding the best estimators and parameters
best_tree = tree_search.best_estimator_
print('best params:', tree_search.best_params_)
print('CV ROC AUC', round(tree_search.best_score_, 3))

# Evaluating on the held-out validation split to capture testing metrics
y_val_proba = best_tree.predict_proba(Xv)[:, 1]
y_val_pred = best_tree.predict(Xv)
cm_val = pd.DataFrame(
  confusion_matrix(yv, y_val_pred),
  index=['True 0', 'True 1'],
  columns=['Pred 0', 'Pred 1']
)
print('Validation ROC AUC', round(roc_auc_score(yv, y_val_proba), 3))
print(cm_val)
tn, fp, fn, tp = confusion_matrix(yv, y_val_pred).ravel()
print(f'Validation TPR: {tp / (tp + fn)}')
print(f'Validation Precision: {tp / (tp + fp)}')
print(f'Validation TNR: {tn / (tn + fp)}')

# The testing metrics for the decision tree is TPR ≈ 0.690, Precision ≈ 0.714, TNR ≈ 0.692 on validation.

```

Q4: 

Logistic Regression Most important predictors

```{python}
# Building the list of feature names from the numeric columns AND the one-hot-encoded categories
features = numeric + list(
  best_logit.named_steps['prep'].named_transformers_['categorical'].get_feature_names_out(categorical)
)

# Combining the feature names with the absolute coefficient values and ranking them by magnitude
importance = (
  pd.DataFrame({
    'feature': features,
    # Use absolute value of coefficients as a proxy for impact
    'importance': np.abs(best_logit.named_steps['logit'].coef_[0])
  })
  .sort_values('importance', ascending=False)
)

print(importance.head(5))

```

Interpretation:
We're able to see that the logistic regression relied most on 'thalach' (maximum heart rate) and the chest pain indicator 'cp_0', with sex_0 also appearing. Other vitals, such as sex_1, thalach, and cp_1 were close follow ups in terms of importance in the logit model.

Decision Tree Classifier Most important predictors

```{python}
features = numeric + list(
  best_tree.named_steps['prep'].named_transformers_['categorical'].get_feature_names_out(categorical)
)

importance = (
  pd.DataFrame({
    'feature': features,
    'importance': best_tree.named_steps['tree'].feature_importances_
  })
  .sort_values('importance', ascending=False)
)

print(importance.head(5))

```

Interpretation:
We're able to see that the decision tree relied most on 'thalach' (maximum heart rate) and the chest pain indicator 'cp_0', with the male indicator also appearing. Other vitals, such as resting blood pressure, cholesterol, and the remaining chest-pain categories rarely appeared in the tree, so they didn't have much importance in this model.


Q5:

```{python}
# Storing ROCs
rocs = []

# Storing all models
models = {
  'kNN': best_knn,
  'Logit': best_logit,
  'Decision Tree': best_tree
}

# Iteratoring through each trained model and its label
for name, model in models.items():
  # Getting the predicted probability of the postitive class on the validation set
  y_scores = model.predict_proba(Xv)[:, 1]
  # computign the false and true positive rates across thresholds
  fpr, tpr, i = roc_curve(yv, y_scores)
  # Storing model's ROC coordinates with the name
  rocs.append(pd.DataFrame({
    'Model': name,
    'FPR': fpr,
    'TPR': tpr
  }))
# Combining all ROC data into single DF for plotting
roc = pd.concat(rocs, ignore_index=True)

( # Initializing the ROC data into a single DF for plotting
  ggplot(roc, aes('FPR', 'TPR', color='Model'))
  + geom_line()
  # adding no-skill reference line
  + geom_abline(intercept=0, slope=1)
  + labs(
    title='ROC Curves for All Models',
    x='False Positive Rate',
    y='True Positive Rate'
  )
  + theme_minimal()
)

```


## Part Two: Metrics
Consider the following metrics:

True Positive Rate or Recall or Sensitivity = Of the observations that are truly Class A, how many were predicted to be Class A?

Precision or Positive Predictive Value = Of all the observations classified as Class A, how many of them were truly from Class A?

True Negative Rate or Specificity or Negative Predictive Value = Of all the observations classified as NOT Class A, how many were truly NOT Class A?

Compute each of these metrics (cross-validated) for your three models (KNN, Logistic Regression, and Decision Tree) in Part One.

```{python}
# Computing the cross validated metrics for the three models
from sklearn.model_selection import KFold, cross_val_predict
from sklearn.metrics import confusion_matrix

# Refitting
# For knn
cv = KFold(n_splits=5, shuffle=True, random_state=42)
y_cv_pred_knn = cross_val_predict(best_knn, Xt, yt, cv=cv)

# For logit
y_cv_pred_logit = cross_val_predict(best_logit, Xt, yt, cv=cv)

# For DTC
y_cv_pred_tree = cross_val_predict(best_tree, Xt, yt, cv=cv)

# Creating a function to calculate tpr, precision, tnr for each model
def cm_metrics(cm):
  tn, fp, fn, tp = cm.ravel()
  tpr = tp / (tp + fn)
  precision = tp / (tp + fp)
  tnr = tn / (tn + fp)
  return tpr, precision, tnr

for label, preds in [
  ('kNN', y_cv_pred_knn),
  ('Logit', y_cv_pred_logit),
  ('Decision Tree', y_cv_pred_tree)
]:
  cm = confusion_matrix(yt, preds)
  tpr, precision, tnr = cm_metrics(cm)
  print(label)
  print(cm)
  print(f'TPR: {tpr}, Precision: {precision}, TNR: {tnr}')
```

*kNN*
TPR:
TPR = 0.77: The kNN model correctly flagged 77.77% of the patients who were truly at risk

Precision:
Precision = 0.7844: when kNN predicts 'at-risk', it's right about 78.44% of the time

True Negative Rate:
TNR = .7524: about 75.24% of the truly not-at-risk patients were correctly predicted as such

*Logistic Regression*
TPR:
TPR = 0.8461: The logit model correctly flagged 84.6% of the patients who were truly at risk

Precision:
Precision = 0.8181: when logit predicts 'at-risk', it's right about 81.8% of the time

True Negative Rate:
TNR = 0.7821: about 78.21% of the truly not-at-risk patients were correctly predicted as such

*Decision Tree*
TPR:
TPR = 0.7521: The decision tree model correctly flagged 75.21% of the patients who were truly at risk

Precision:
Precision = 0.7652: when decision predicts 'at-risk', it's right about 76.52% of the time

True Negative Rate:
TNR = 0.7326: about 73.26% of the truly not-at-risk patients were correctly predicted as such



## Part Three: Discussion

Part Three: Discussion
Suppose you have been hired by a hospital to create classification models for heart attack risk.

The following questions give a possible scenario for why the hospital is interested in these models. For each one, discuss:

Which metric(s) you would use for model selection and why.

Which of your final models (Part One Q1-3) you would recommend to the hospital, and why.

What score you should expect for your chosen metric(s) using your chosen model to predict future observations.

Q1
The hospital faces severe lawsuits if they deem a patient to be low risk, and that patient later experiences a heart attack.

Q2
The hospital is overfull, and wants to only use bed space for patients most in need of monitoring due to heart attack risk.

Q3
The hospital is studying root causes of heart attacks, and would like to understand which biological measures are associated with heart attack risk.

Q4
The hospital is training a new batch of doctors, and they would like to compare the diagnoses of these doctors to the predictions given by the algorithm to measure the ability of new doctors to diagnose patients.


### Q1
When the cost of missing a high-risk patient is critical, I would optimize for recall/TPR so that we minimize false negatives. Among the fitted models I made, the logistic regression produced the highest cross-validiated TPR(~0.85 - part 2), so that is the model I would deploy. Based on cross validation I would tell the hospitals to expect recall in the mid-0.80s when future patients resemble the training set. But, the separate validation fold showed recall closer to 0.59, so I would communicate an expected range of about 0.6-0.8 and to keep monitoring this metric in production.

### Q2
Here, the hospital only wants to flag patients if it is highly confident that they truly need scarce beds, which would mean that precision should drive the model selection. Again, the logistic regression model led the other models with cross-validated precision around 0.82, so I would start there. In addition to this, it may be worthwhile to raise the classification threshold to trade the recall for fewer false positives. The validation data delivered precision of around 0.68, so I would set the expectation that the future precision would fall between the high 0.60s and low 0.80s.

### Q3
Studying root causes requires interpretable parameters in addition to good discrimination, so it would be worthwhile to evaluate the models based on ROC AUC and by how clearly the explain the feature effects. Logistic regression is the best model suited for this, as it's standardized coefficients can be translated into odds ratios, which would help the clinicians quantify how maximum heart rate and chest pain categories influence risk. With a CV ROC AUC near 0.89 (and 0.72 on the validation set), we can expect high -0.80s on the future data while still maintaining scientific interpretability.

### Q4
When we compare trainees to the algorithm we want equal weight on correctly identifying both risk classes, so I would use a balanced accuracy (average of TPR and TNR) as the selection metric. The decision tree is the best choice here. The if/then structure mirrors how humans reason, so trainees can contrast their thought process with the tree's splits. From Part 2, the tree achieved a CV TPR of 0.75, and TNR of 0.73, so I would tell the hospital to expect a balanced accuracy of roughly (0.75 + 0.73) / 2 = 0.74 on the future cohorts when comparing trainee diagnoses to the model's predictions.


Part Four: Validation
Before sharing the dataset with you, I set aside a random 10% of the observations to serve as a final validation set.

ha_validation = pd.read_csv("https://www.dropbox.com/s/jkwqdiyx6o6oad0/heart_attack_validation.csv?dl=1")

Use each of your final models in Part One Q1-3, predict the target variable in the validation dataset.

For each, output a confusion matrix, and report the ROC AUC, the precision, and the recall.

Compare these values to the cross-validated estimates you reported in Part One and Part Two. Did our measure of model success turn out to be approximately correct for the validation data?

```{python}
# Importing new data
ha2 = pd.read_csv('ha2.csv')
ha2.info()
```

```{python}
# Defining X's and y
X_new = ha2.drop('output',axis=1)
y_new = ha2['output']
```

```{python}
from sklearn.metrics import precision_score, recall_score
# For each model, outputting a confusion matrix, reporting ROC AUC, precision, and recall
summary_validation = []

final_models = [
  ('kNN', best_knn, knn_search.best_score_),
  ('Logit', best_logit, logit_search.best_score_),
  ('Decision Tree', best_tree, tree_search.best_score_)

]

for name, model, cv_auc in final_models:
  print(f'{name}')
  # Predicted labels and class-1 probabilities on the validation data
  y_pred = model.predict(X_new)
  y_proba = model.predict_proba(X_new)[:, 1] # passing in class 1
  # Confusion matrix for quick error inspection
  cm = confusion_matrix(y_new, y_pred)
  print(pd.DataFrame(
    cm,
    index=['True 0', 'True 1'],
    columns=['Pred 0', 'Pred 1']
  ))
  # Finding out the validation metrics
  roc = roc_auc_score(y_new, y_proba)
  precision = precision_score(y_new, y_pred)
  recall = recall_score(y_new, y_pred)
  print(f'Validation ROC AUC: {roc}')
  print(f'Validation Precision: {precision}')
  print(f"Validation Recall: {recall}")

  # Comparing values to the cross validated estimates of ROC AUC reported in parts 1 and part 2
  summary_validation.append({
    'Model': name,
    'CV ROC AUC (p1)': round(cv_auc, 3),
    'Validation ROC AUC': round(roc, 3),
    'Validation Precision': round(precision, 3),
    'Validation Recall': round(recall, 3)
  })

# Tabular summary to compare models
pd.DataFrame(summary_validation)

```

### kNN
The kNN model performed noticeably worse on the validation data compared to it's cross-validated performance. While the CV ROC AUC was 0.856, the validation ROC AUC dropped down to 0.782, which indicates a weaker discrimination ability on newer patients. Precision remained relatively high at ~0.83, but recall decreased to ~0.53, which means that the model missed nearly half of truly high-risk patients in the validation set. Because the recall was much lower than the CV, (~0.78), the kNN model appears to have been overfitted and doesn't generalize well to new data.

### Logistic Regression
Overall, the logit model generalizes more consistently. It's validation ROC AUC of 0.856 was very close to the CV estimate of 0.893, which shows that the model held it's discriminatory power over new data. The precision (0.81) and recall (0.684) were decently lower than their cross-validated values, but still relatively strong. Overall, the logistic regression model performed similarly on both the CV and validation data, which highlights it's stability and reliability for predicting heart attack risk.

### Decision Tree
The decision tree performed better on validation data, where it's validation ROC AUC (0.883) exceeded it's CV ROC AUC (0.814). It achieved very high precision (0.929) with approximately the same recall as logistic regression (0.68). The tree produced only one false positive and correctly identified most high-risk patients. This suggests that the tuned tree did not overfit and actually generalized slightly better than the cross-validated indicated.

### Overall Comparison
Across all the models, the validation results were broadly consistent with the cross -validated trends, though the magnitude differed. Logistic regression showed the most stable performance, and the decision tree unexpectedly improved on the validation set. The kNN model performed noticeably worse than its CV estimates. The highlights the CV metrics were reasonable but slightly optimistic for kNN and slightly pessimistic for the decision tree.


Part Five: Cohen’s Kappa
Another common metric used in classification is Cohen’s Kappa.

Use online resources to research this measurement. Calculate it for the models from Part One, Q1-3, and discuss reasons or scenarios that would make us prefer to use this metric as our measure of model success. Do your conclusions from above change if you judge your models using Cohen’s Kappa instead? Does this make sense?


```{python}
from sklearn.metrics import cohen_kappa_score

# Comparing the cross-validated predictions to true labels with Cohen's Kappa
kappas = []

for name, preds in [
  ('kNN', y_cv_pred_knn),
  ('Logit', y_cv_pred_logit),
  ('Decision Tree', y_cv_pred_tree)
]:
  kappa = cohen_kappa_score(yt, preds)
  kappas.append({'Model': name, 'Cohen Kappa': round(kappa, 3)})
  print(f'{name} Kappa: {kappa}')

```

Discussion:
- Why we're using Kappa: Cohen's kappa score adjusts for agreement expects by chance, essentially making it easier when there are class imbalances or when we want a single score or when we want a single score that penalizes both error types while accounting for the baseline prevalence of heart attacks. This is the type of situation in P3 Q4 where the hospital compares the model's diagnoses with the trainees. With interpreting our results, we find that when K=0.4-0.6, it indicates moderate agreement, where k>0.6 is substantial. In the kNN (K=0.53) and the decision tree (k=0.48), they sit in the moderate band, which means that they beat chance but still disagree with the true diagnoses relatively often. However, Logistic regression hits k=0.63, which passes into substantial agreement and indicates that it stays aligned with the actual labels more consistently. This doesn't change our conclusions, as logit > kNN > tree mirrors what we saw with our ROC AUC, precision, and recall, so logistic regression remains the recommended model. This makes sense, as chance-adjusted agreement validates that logit generalizes best, while kNN and the decision tree continue to lag, especially on the validation fold.