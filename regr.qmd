---
title: "GSB 544 - Regression Project"
author: "Sahil Bains"
format:
  html:
    embed-resources: true
    code-fold: true
echo: true
---

This script applies several regularized polynomial regression models covered in GSB 544 - Ridge, Lasso, and Elastic Net with polynomial feature expansion to the Ames housing dataset. External Assistance (AI) was used for preprocess design, tuning hyperparameters for model optimization, organizing diagnostics, plotting, and saving predictions in the required format.


## Setup

```{python}
# Import required libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import GridSearchCV, KFold
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures
from sklearn.pipeline import Pipeline
from sklearn.linear_model import Ridge, Lasso, ElasticNet
from sklearn.metrics import make_scorer, mean_squared_error

# Loading training dataset
train_df = pd.read_csv("regrs_train.csv")

# Loading test dataset
test_df = pd.read_csv("regrs_test.csv")

# Print shapes and summary statistics of training dataset
print(f"Train shape: {train_df.shape}")
print(f"Test shape: {test_df.shape}")
print(train_df["SalePrice"].describe())
```

## Preprocessing

```{python}

# Remove outliers (Gr Liv Area > 4000)
# These are partial sales or extreme outliers that distort linear models
train_df = train_df[train_df['Gr Liv Area'] <= 4000]

# Define target variable (Log Transformed) and features
y = np.log(train_df['SalePrice'])
X = train_df.drop(['SalePrice', 'PID'], axis=1)

# Identifty numeric columns
numeric_cols = X.select_dtypes(include=["int64", "float64"]).columns

# Identify categorical columns
categorical_cols = X.select_dtypes(include=["object"]).columns

# 1. Log-transform skewed features
skewed_cols = ["Lot Area", "Gr Liv Area", "TotRms AbvGrd"]

# Check if columns exist before transforming (robustness)
existing_skewed = [col for col in skewed_cols if col in X.columns]
for col in existing_skewed:
    # Use log1p to handle zeros safely
    X[col] = np.log1p(X[col])

# Fill numeric missing values with median
X[numeric_cols] = X[numeric_cols].fillna(X[numeric_cols].median())

# Collapse rare categorical levels into 'Other' to stabilize estimates
def collapse_rare(series, threshold=10):
    counts = series.value_counts()
    rare = counts[counts < threshold].index
    if len(rare) > 0:
        return series.replace(rare, "Other")
    return series

for col in categorical_cols:
    # Fill missing first
    X[col] = X[col].fillna(X[col].mode()[0])
    # Collapse rare
    X[col] = collapse_rare(X[col])

# Store numeric feature list
numeric_features = list(numeric_cols)

# Store categorical feature list
categorical_features = list(categorical_cols)

# Build preprocessing pipeline
preprocessor = ColumnTransformer(
    transformers=[
        ("num", StandardScaler(), numeric_features),
        ("cat", OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features),
    ]
)

# Define 5-fold cross-validation
cv = KFold(n_splits=5, shuffle=True, random_state=42)

# Define RMSE scorer in order to return positive RMSE values
def rmse_scorer(y_true, y_pred):
    # Return negative RMSE
    return -np.sqrt(mean_squared_error(y_true, y_pred))

# Wrap custom scorer - defining 'smallest' rmse as best
scorer = make_scorer(rmse_scorer, greater_is_better=True)
```

## Model Training & Evaluation

### 5. Polynomial Features with Ridge

```{python}
# Create Polynomial + Ridge pipeline with interaction_only for speed
poly_ridge_pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('poly', PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)),
    ('regressor', Ridge())
])

# Define Ridge hyperparameters - A+ Boost Plan Refinement
poly_ridge_params = {"regressor__alpha": [60, 80, 100, 120, 140]}

# Run grid search on Polynomial Ridge
grid_poly_ridge = GridSearchCV(poly_ridge_pipeline, poly_ridge_params, cv=cv, scoring=scorer, n_jobs=-1)

# Fit Polynomial Ridge model
grid_poly_ridge.fit(X, y)

# Store RMSE score
poly_ridge_score = -grid_poly_ridge.best_score_

# Print results
print("Polynomial Ridge (interactions) RMSE:", round(poly_ridge_score, 4))
print("Best params:", grid_poly_ridge.best_params_)
```


### 10. Elastic Net with Polynomial Interactions

```{python}
# Create Polynomial + Elastic Net pipeline
poly_elasticnet_pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('poly', PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)),
    ('regressor', ElasticNet(max_iter=10000))
])

# Tune for polynomial features - higher alpha needed
poly_elasticnet_param_grid = {
    'regressor__alpha': [0.001, 0.01, 0.1, 1, 10, 50, 100],
    'regressor__l1_ratio': [0.1, 0.5, 0.7, 0.9, 0.95, 0.99]
}

# Perform grid search with cross-validation
grid_poly_elasticnet = GridSearchCV(poly_elasticnet_pipeline, poly_elasticnet_param_grid, cv=cv, 
                                    scoring=scorer, n_jobs=-1)
grid_poly_elasticnet.fit(X, y)

poly_elasticnet_score = -grid_poly_elasticnet.best_score_  # Convert to positive RMSE
print(f"Best Poly Elastic Net Params: {grid_poly_elasticnet.best_params_}")
print(f"Poly Elastic Net CV RMSE (log): {poly_elasticnet_score:.4f}")
```

### 11. Polynomial Lasso (Interactions Only)

```{python}
# Create Polynomial + Lasso pipeline
poly_lasso_pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('poly', PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)),
    ('regressor', Lasso(max_iter=10000))
])

# Tune alpha for Lasso with polynomial features
poly_lasso_param_grid = {
    'regressor__alpha': [0.00001, 0.00005, 0.0001, 0.0005, 0.001, 0.005, 0.01]
}

# Perform grid search with cross-validation
grid_poly_lasso = GridSearchCV(poly_lasso_pipeline, poly_lasso_param_grid, cv=cv, 
                               scoring=scorer, n_jobs=-1)
grid_poly_lasso.fit(X, y)

poly_lasso_score = -grid_poly_lasso.best_score_  # Convert to positive RMSE
print(f"Best Poly Lasso Params: {grid_poly_lasso.best_params_}")
print(f"Poly Lasso CV RMSE (log): {poly_lasso_score:.4f}")
```

## Model Comparison & Selection

```{python}
results = {
    "Polynomial Ridge (interactions)": poly_ridge_score,
    "Poly Elastic Net": poly_elasticnet_score,
    "Poly Lasso": poly_lasso_score,
}

# Print sorted performance table
print("\nModel Performance (RMSE):")
for model, score in sorted(results.items(), key=lambda x: x[1]):
    print(f"{model:30s}: {score:.4f}")

# Select best model name
best_model_name = min(results, key=results.get)
print("\nBest model:", best_model_name)

# Map model name to fitted estimator
model_map = {
    "Polynomial Ridge (interactions)": grid_poly_ridge.best_estimator_,
    "Poly Elastic Net": grid_poly_elasticnet.best_estimator_,
    "Poly Lasso": grid_poly_lasso.best_estimator_,
}

# Retrieve best model
best_model = model_map[best_model_name]
```

## Model Scores and Diagnostics
```{python}
for model, score in sorted(results.items(), key=lambda x: x[1]):
    print(f"{model:35s}: {score:.5f}")

# Print best model selected
print(f"Best model: {best_model_name}")
print(f"Best model's RMSE: {results[best_model_name]:.5f}")


```

## Practical Model Accuracy (Diagnostics)

```{python}
from sklearn.model_selection import cross_val_predict
import matplotlib.pyplot as plt
import matplotlib.ticker as ticker
import numpy as np

# CV predictions on original (un-logged) scale
y_pred_cv_log = cross_val_predict(best_model, X, y, cv=cv, n_jobs=-1)
y_pred_cv_real = np.exp(y_pred_cv_log)
y_real = np.exp(y)

# Practical accuracy diagnostics
mae = np.mean(np.abs(y_real - y_pred_cv_real))
pct_error = np.abs(y_real - y_pred_cv_real) / y_real
acc_10 = (pct_error <= 0.10).mean() * 100

print(f"Mean Absolute Error: ${mae:,.2f}")
print(f"Predictions within 10% of actual: {acc_10:.1f}%")

# Designing plot for predicted versus actual values
fig, ax = plt.subplots(figsize=(8, 5))

# Scatter points: smaller, softer, more transparent
ax.scatter(
    y_real,
    y_pred_cv_real,
    alpha=0.3,
    color="#5c6c7f",
    s=12,
    edgecolors="none"
)

# Integrating perfect prediction line
min_val = min(y_real.min(), y_pred_cv_real.min())
max_val = max(y_real.max(), y_pred_cv_real.max())
ax.plot(
    [min_val, max_val],
    [min_val, max_val],
    linestyle="--",
    color="#e85b5b",
    linewidth=1.2
)

# Keep axes tight around the data
ax.set_xlim(min_val, max_val)
ax.set_ylim(min_val, max_val)

# Dollar formatting for both axes, with fewer ticks
formatter = ticker.FuncFormatter(lambda x, p: f'${x/1000:.0f}k')
ax.xaxis.set_major_formatter(formatter)
ax.yaxis.set_major_formatter(formatter)
ax.xaxis.set_major_locator(ticker.MaxNLocator(5))
ax.yaxis.set_major_locator(ticker.MaxNLocator(5))

# Labels & title
ax.set_xlabel("Actual Sale Price", fontsize=11)
ax.set_ylabel("Predicted Sale Price", fontsize=11)
ax.set_title("Predicted vs Actual Prices", fontsize=12, pad=10)

# Very light grid & clean spines
ax.grid(True, axis="both", linestyle="--", alpha=0.15)
for spine in ["top", "right"]:
    ax.spines[spine].set_visible(False)
for spine in ax.spines.values():
    spine.set_visible(False)

plt.tight_layout(pad=2)
plt.show()

## Key Predictors in Best Model

# Fit model to get coefficients
best_model.fit(X, y)

# Extract feature names from pipeline (Preprocessor -> Poly)
feature_names = best_model[:-1].get_feature_names_out()

# Get coefficients
coefs = best_model.named_steps['regressor'].coef_

# Create DataFrame
imp_df = pd.DataFrame({'Feature': feature_names, 'Coefficient': coefs})
imp_df['Abs_Coeff'] = imp_df['Coefficient'].abs()

# Sort and Display Top 10
top_features = imp_df.sort_values(by='Abs_Coeff', ascending=False).head(10)

print(top_features[['Feature', 'Coefficient']])
```

## Final Predictions

```{python}
# Retrain best model on full dataset
best_model.fit(X, y)

# Prepare test dataset (drop PID)
X_test = test_df.drop(["PID"], axis=1)

# Apply Same Preprocessing to Test Set
# Log transform test features
existing_skewed_test = [col for col in skewed_cols if col in X_test.columns]
for col in existing_skewed_test:
    X_test[col] = np.log1p(X_test[col])

# Fill numeric missing values with median
X_test[numeric_cols] = X_test[numeric_cols].fillna(X_test[numeric_cols].median())

# Fill categorical missing values
for col in categorical_cols:
    X_test[col] = X_test[col].fillna(X_test[col].mode()[0])
    # Apply collapse rare categories
    X_test[col] = collapse_rare(X_test[col])

# Predict log-sale price
test_pred_log = best_model.predict(X_test)

# Convert back to original scale
test_pred = np.exp(test_pred_log)

# Build submission file
submission = pd.DataFrame({"PID": test_df["PID"], "SalePrice": test_pred})

# Save submission file
submission.to_csv("submissionr.csv", index=False)

# Show preview
print(submission.head())

# Confirm save
print("Saved to submissionr.csv")
