---
title: GSBS544 Lab 6
author: Sahil Bains
format:
  html:
    embed-resources: true
    code-fold: true
echo: true
---
Github Link: https://github.com/sbains2/GSBS544

```{python}
import pandas as pd
import numpy as np
from sklearn.pipeline import Pipeline
from sklearn.compose import make_column_selector, ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.metrics import r2_score, mean_squared_error
import math
from plotnine import *

# Importing the data and doing some checks
df = pd.read_csv('hitters.csv')
df.info()
df.describe()

```


```{python}
# Removing NA's in the salary - dropped about 60 rows
df = df.dropna(subset=['Salary'])
df
```

Part I: Different Model Specs
A. 

i. Regression without regularization
Create a pipeline that includes all the columns as predictors for Salary, and performs ordinary linear regression

```{python}

# Defining X and y
X = df.drop(columns=['Salary'])
y = df['Salary']

# Defining numeric columns
numeric = X.select_dtypes(include=[np.number]).columns

# Setting up column transformer
ct = ColumnTransformer(
    [ # Dummifying league, division, and new league
        ('dummify', OneHotEncoder(sparse_output=False, drop='first', handle_unknown='ignore'), ['League', 'Division', 'NewLeague']),
        # Standardizing all numeric variables
        ('standardize', StandardScaler(), numeric)
    ],
    remainder='drop'
)

# Setting up pipeline
pipe = Pipeline(
  [("preprocessing", ct),
  ("linear_regression", LinearRegression())]
)

# Splitting training and testing variables
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

pipe.fit(X_train, y_train)

r2 = pipe.score(X_test, y_test)
print(r2)

```

ii. Fit this pipeline to the full dataset, and interpret a few of the most important coefficients.

```{python}
# Fitting pipeline to entire dataset
fitted = pipe.fit(X,y)

# Getting all the feature names from the fitted column transformer steps - dummify and standardizing variables
features = fitted.named_steps['preprocessing'].get_feature_names_out()

# Getting all the coefficients
coeffs = fitted.named_steps['linear_regression'].coef_

# Initializing dataframe to put coefficients in
comparing_coeffs = pd.DataFrame()

# Formatting coefficients and features in dataframe
comparing_coeffs['features'] = features
comparing_coeffs["LR"] = coeffs


comparing_coeffs
```

Interpretation:
A few of the most important coefficients are Career runs, number of times at bat during career, and number of times at bat. Where for every 1 SD increase in these variables, the salary increases by ~480$, ~-391$, ~-291$ respectively

iii. Use cross-validation to estimate the MSE you would expect if you used this pipeline to predict 1989 salaries.

```{python}
# Finding cross validation scores - r2 and mse
r2scores = cross_val_score(pipe, X, y, cv=5, scoring='r2')
mse = -cross_val_score(pipe, X, y, cv=5, scoring='neg_mean_squared_error').mean()


print('CV R2 scores:', r2scores.mean(), 'mean squared error:', mse) # 121136.31

```

If we utilized this pipeline, using the current cross validation scores available, we can estimate a MSE of 112677.23, indicating the model error would, on average, give us salary estimates that are $112677.23 off.

B. Ridge regression
Create a pipeline that includes all the columns as predictors for Salary, and performs ordinary ridge regression

```{python}

# Creating a pipeline that performs ordinary ridge regression
ridge_pipe = Pipeline(
    [
        ('standardizing', ct), # using the original standardizing column transformer
        ('ridge_regression', Ridge(alpha=1))
    ]
)

ridge_pipe_fitted = ridge_pipe.fit(X_train, y_train)

```

Use cross-validation to tune the lambda hyperparameter.

```{python}

# Defining the alpha values
alphas = {'ridge_regression__alpha': np.array([1000, 100, 10, 1, 0.1, 0.01, 0.001, 0.0001])}

# Defining the grid search cross validation on the ridge pipeline 
gscv = GridSearchCV(ridge_pipe, param_grid=alphas, cv = 5, scoring='neg_mean_squared_error')

fitted_grid = gscv.fit(X,y)

# Creating a dataframe of the differing values of alphas and the scores of the fitted grid search cross val scores
print(pd.DataFrame(data = {"alphas": np.array([1000, 100, 10, 1, 0.1, 0.01, 0.001, 0.0001]), "scores": fitted_grid.cv_results_['mean_test_score']}))
print(-gscv.best_score_)

```

Fit the pipeline with your chosen lambda to the full dataset, and interpret a few of the most important coefficients.

```{python}
# Extracting best lambda
best_lambda = fitted_grid.best_params_['ridge_regression__alpha']
best_lambda

ridge_full = fitted_grid.best_estimator_
```

```{python}

# Creating column to see all the features along with the ridge regression coefficents
comparing_coeffs['features'] = ridge_full.named_steps['standardizing'].get_feature_names_out()
comparing_coeffs['ridge'] = ridge_full.named_steps['ridge_regression'].coef_

comparing_coeffs
```

In interpreting some of the most important coefficients, we can see that for every 1 increase standard deviation in number of runs in a players career, the salary is expected to increase by $321.88. Another significant factor would be the number of hits in 1986, where every standard deviation increase in the number of hits translates to an increase in the salary by $286.92.

Report the MSE you would expect if you used this pipeline to predict 1989 salaries.

```{python}

y_preds = ridge_pipe_fitted.predict(X_test)

print('mean squared error:', mean_squared_error(y_test, y_preds)) # 126551.99

```

C. Lasso Regression
Create a pipeline that includes all the columns as predictors for Salary, and performs ordinary lasso regression

```{python}
# Creating a pipeline that performs ordinary lasso regression
lasso_pipe = Pipeline(
    [
        ('standardizing', ct), # using the original standardizing column transformer
        ('lasso_regression', Lasso(alpha=1))
    ]
)

lasso_pipe_fitted = lasso_pipe.fit(X_train, y_train)
```

Use cross-validation to tune the lambda hyperparameter.

```{python}

# Defining the alpha values
alphas = {'lasso_regression__alpha': np.array([1000, 100, 10, 1, 0.1, 0.01, 0.001, 0.0001])}

# Defining the grid search cross validation on the ridge pipeline 
gscv = GridSearchCV(lasso_pipe, param_grid=alphas, cv = 5, scoring='neg_mean_squared_error')

fitted_grid = gscv.fit(X,y)

# Finding the best estimator for the lasso regression
lasso_best_estimator = fitted_grid.best_params_['lasso_regression__alpha']

# Fitting the lasso pipe to the full dataset
lasso_full = fitted_grid.best_estimator_

# Creating a dataframe of the differing values of alphas and the scores of the fitted grid search cross val scores
print(pd.DataFrame(data = {"alphas": np.array([1000, 100, 10, 1, 0.1, 0.01, 0.001, 0.0001]), "scores": fitted_grid.cv_results_['mean_test_score']}), -gscv.best_score_)

# 119761.58
```

Fit the pipeline with your chosen lambda to the full dataset, and interpret a few of the most important coefficients.

```{python}
comparing_coeffs
```

```{python}
lasso_coeffs = lasso_full.named_steps['lasso_regression'].coef_

comparing_coeffs['lasso'] = lasso_full.named_steps['lasso_regression'].coef_

comparing_coeffs
```
In interpreting the most important coefficients after fitting a lasso regression, we see that a 1 sd increase in the number of walks that a player has a $9.95 increase in a players salary. In addition to this, for every 1 sd increase in the number of put outs that a player has, a player's salary increases by $7.57.

Report the MSE you would expect if you used this pipeline to predict 1989 salaries.

```{python}

y_preds = lasso_pipe_fitted.predict(X_test)

print('mean squared error:', mean_squared_error(y_test, y_preds))

# 126212.36
```

D. Elastic Net
Create a pipeline that includes all the columns as predictors for Salary, and performs ordinary elastic net regression

```{python}
# Creating a pipeline that performs ordinary elastic regression
elastic_pipe = Pipeline(
    [
        ('standardizing', ct), # using the original standardizing column transformer
        ('elastic_regression', ElasticNet(alpha=1, l1_ratio=0.5))
    ]
)

elastic_pipe_fitted = elastic_pipe.fit(X_train, y_train)

```

Use cross-validation to tune the lambda and alpha hyperparameters.

```{python}

# Defining the alpha and l1_ratio values
tuning = {'elastic_regression__alpha': np.array([1, 0.1, 0.01, 0.001, 0.0001]),
          'elastic_regression__l1_ratio': np.arange(0.2, 1.0, 0.2)}

# Defining the grid search cross validation on the ridge pipeline 
gscv = GridSearchCV(elastic_pipe, param_grid=tuning, cv = 5, scoring='neg_mean_squared_error')

fitted_grid = gscv.fit(X,y)

# Creating a dataframe of the differing values of alphas and the scores of the fitted grid search cross val scores
print(pd.DataFrame({
    'alpha': fitted_grid.cv_results_['param_elastic_regression__alpha'].astype(float),
    'l1_ratio': fitted_grid.cv_results_["param_elastic_regression__l1_ratio"].astype(float),
    'mean_negative_mse': fitted_grid.cv_results_["mean_test_score"]
}).sort_values('mean_negative_mse', ascending=False), -gscv.best_score_)

```

Fit the pipeline with your chosen hyperparameters to the full dataset, and interpret a few of the most important coefficients.


```{python}

elastic_full = fitted_grid.best_estimator_

comparing_coeffs['elastic'] = elastic_full.named_steps['elastic_regression'].coef_

comparing_coeffs
```

Interpretation:
When interpreting the coefficients for the elastic net regression methods, for every 1 SD increase in the number of putouts that a player has, their salary goes up by $41.04. Additionally, when the number of runs batted increases by 1 SD, we see that a player's salary is expected to increase on average by $35.65.


Report the MSE you would expect if you used this pipeline to predict 1989 salaries.

```{python}

y_preds = elastic_pipe_fitted.predict(X_test)

print('mean squared error:', mean_squared_error(y_test, y_preds)) # 148422.19

```


Part II. Variable Selection
Based on the above results, decide on:

Which numeric variable is most important.

_Most important numeric variable: Number of putouts that a player has._

Which five numeric variables are most important

_1. Number of hits (Hits)_
_2. Number of times at bat (AtBat)_
_3. Number of runs batted (RBI)_
_4. Number of walks (Walks)_
_5. Number of Put outs (put outs)_

Which categorical variable is most important

_The most important categorical variable is if the player plays in the West division. (vs East baseline)_

For each of the four model specifications, compare the following possible feature sets:

Using only the one best numeric variable.

```{python}
# Running linear regression for only number of putouts a player has

# Defining new X
X = df[['PutOuts']]

# Setting up new column transformer
ct = ColumnTransformer(
    [   # Standardizing all numeric variables
        ('standardize', StandardScaler(), ['PutOuts'])
    ],
    remainder='drop'
)

# Setting up pipeline
lr_pipe = Pipeline(
  [("preprocessing", ct),
  ("linear_regression", LinearRegression())]
)

# Fitting pipeline to entire dataset
lr_fitted = lr_pipe.fit(X,y)

# Getting all the feature names from the fitted column transformer steps - dummify and standardizing variables
features = lr_fitted.named_steps['preprocessing'].get_feature_names_out()

# Getting all the coefficients
coeffs = lr_fitted.named_steps['linear_regression'].coef_

# Initializing dataframe to put coefficients in
comparing_coeffs = pd.DataFrame()

# Formatting coefficients and features in dataframe
comparing_coeffs['features'] = features
comparing_coeffs["LR"] = coeffs

print(comparing_coeffs)

```

```{python}
# Running ridge regression for only number of putouts a player has

# Setting up pipeline
ridge_pipe = Pipeline(
  [("preprocessing", ct),
  ("ridge_regression", Ridge())]
)

# Defining the alpha values
alphas = {'ridge_regression__alpha': np.array([1000, 100, 10, 1, 0.1, 0.01, 0.001, 0.0001])}

# Defining the grid search cross validation on the ridge pipeline 
gscv = GridSearchCV(ridge_pipe, param_grid=alphas, cv = 5, scoring='neg_mean_squared_error')

fitted_grid = gscv.fit(X,y)

best_alpha = fitted_grid.best_params_['ridge_regression__alpha']

ridge_pipe = fitted_grid.best_estimator_

# Fitting pipeline to entire dataset
ridge_fitted = ridge_pipe.fit(X,y)

# Getting all the feature names from the fitted column transformer steps - dummify and standardizing variables
features = ridge_fitted.named_steps['preprocessing'].get_feature_names_out()

# Getting all the coefficients
coeffs = ridge_fitted.named_steps['ridge_regression'].coef_

# Formatting coefficients and features in dataframe
comparing_coeffs["Ridge"] = coeffs

print(comparing_coeffs, -gscv.best_score_) # 193417.80
```

```{python}
# Running lasso regression for only number of putouts a player has

# Setting up pipeline
lasso_pipe = Pipeline(
  [("preprocessing", ct),
  ("lasso_regression", Lasso())]
)

# Defining the alpha values
alphas = {'lasso_regression__alpha': np.array([1000, 100, 10, 1, 0.1, 0.01, 0.001, 0.0001])}

# Defining the grid search cross validation on the ridge pipeline 
gscv = GridSearchCV(lasso_pipe, param_grid=alphas, cv = 5, scoring='neg_mean_squared_error')

fitted_grid = gscv.fit(X,y)

best_alpha = fitted_grid.best_params_['lasso_regression__alpha']

lasso_pipe = fitted_grid.best_estimator_

# Fitting pipeline to entire dataset
lasso_fitted = lasso_pipe.fit(X,y)

# Getting all the feature names from the fitted column transformer steps - dummify and standardizing variables
features = lasso_fitted.named_steps['preprocessing'].get_feature_names_out()

# Getting all the coefficients
coeffs = lasso_fitted.named_steps['lasso_regression'].coef_

# Formatting coefficients and features in dataframe
comparing_coeffs["Lasso"] = coeffs

print(comparing_coeffs, -gscv.best_score_) # 194207.36
```

```{python}
# Running elastic net regression for only number of putouts a player has

# Setting up pipeline
elastic_pipe = Pipeline(
  [("preprocessing", ct),
  ("elastic_regression", ElasticNet(max_iter=1000))]
)

# Defining the alpha and l1 ratio values
tuning = {'elastic_regression__alpha': np.array([1, 0.1, 0.01, 0.001, 0.0001]),
          'elastic_regression__l1_ratio': np.arange(0.2, 1.0, 0.2)}

# Defining the grid search cross validation on the ridge pipeline 
gscv = GridSearchCV(elastic_pipe, param_grid=tuning, cv = 5, scoring='neg_mean_squared_error')

fitted_grid = gscv.fit(X,y)

best_alpha_elastic = fitted_grid.best_params_['elastic_regression__alpha']

elastic_pipe = fitted_grid.best_estimator_

# Fitting pipeline to entire dataset
elastic_fitted = elastic_pipe.fit(X,y)

# Getting all the feature names from the fitted column transformer steps - dummify and standardizing variables
features = elastic_fitted.named_steps['preprocessing'].get_feature_names_out()

# Getting all the coefficients
coeffs = elastic_fitted.named_steps['elastic_regression'].coef_

# Formatting coefficients and features in dataframe
comparing_coeffs["elastic"] = coeffs

print(comparing_coeffs, -gscv.best_score_) # 193305.87
```

Using only the five best variables.

```{python}
# Running linear regression for the best numerical variables.

# Defining new X
X = df[['Hits', 'RBI', 'AtBat', 'Walks', 'PutOuts']]

# Setting up new column transformer
ct = ColumnTransformer(
    [   # Standardizing all numeric variables
        ('standardize', StandardScaler(), ['Hits', 'RBI', 'AtBat', 'Walks', 'PutOuts'])
    ],
    remainder='drop'
)

# Setting up pipeline
lr_pipe = Pipeline(
  [("preprocessing", ct),
  ("linear_regression", LinearRegression())]
)

# Fitting pipeline to entire dataset
lr_fitted = lr_pipe.fit(X,y)

# Getting all the feature names from the fitted column transformer steps - dummify and standardizing variables
features = lr_fitted.named_steps['preprocessing'].get_feature_names_out()

# Getting all the coefficients
coeffs = lr_fitted.named_steps['linear_regression'].coef_

# Initializing dataframe to put coefficients in
comparing_coeffs = pd.DataFrame()

# Formatting coefficients and features in dataframe
comparing_coeffs['features'] = features
comparing_coeffs["LR"] = coeffs

print(comparing_coeffs)

```

```{python}
# Running ridge regression for for the best numerical variables.

# Setting up pipeline
ridge_pipe = Pipeline(
  [("preprocessing", ct),
  ("ridge_regression", Ridge())]
)


# Defining the alpha values
alphas = {'ridge_regression__alpha': np.array([1000, 100, 10, 1, 0.1, 0.01, 0.001, 0.0001])}

# Defining the grid search cross validation on the ridge pipeline 
gscv = GridSearchCV(ridge_pipe, param_grid=alphas, cv = 5, scoring='neg_mean_squared_error')

fitted_grid = gscv.fit(X,y)

best_alpha = fitted_grid.best_params_['ridge_regression__alpha']

ridge_pipe = fitted_grid.best_estimator_

# Fitting pipeline to entire dataset
ridge_fitted = ridge_pipe.fit(X,y)

# Getting all the feature names from the fitted column transformer steps - dummify and standardizing variables
features = ridge_fitted.named_steps['preprocessing'].get_feature_names_out()

# Getting all the coefficients
coeffs = ridge_fitted.named_steps['ridge_regression'].coef_

# Formatting coefficients and features in dataframe
comparing_coeffs["Ridge"] = coeffs

print(comparing_coeffs, -gscv.best_score_) # 152065.80
```

```{python}
# Running lasso regression for for the best numerical variables.


# Setting up pipeline
lasso_pipe = Pipeline(
  [("preprocessing", ct),
  ("lasso_regression", Lasso())]
)


# Defining the alpha values
alphas = {'lasso_regression__alpha': np.array([1000, 100, 10, 1, 0.1, 0.01, 0.001, 0.0001])}

# Defining the grid search cross validation on the ridge pipeline 
gscv = GridSearchCV(lasso_pipe, param_grid=alphas, cv = 5, scoring='neg_mean_squared_error')

fitted_grid = gscv.fit(X,y)

best_alpha = fitted_grid.best_params_['lasso_regression__alpha']

lasso_pipe = fitted_grid.best_estimator_

# Fitting pipeline to entire dataset
lasso_fitted = lasso_pipe.fit(X,y)

# Getting all the feature names from the fitted column transformer steps - dummify and standardizing variables
features = lasso_fitted.named_steps['preprocessing'].get_feature_names_out()

# Getting all the coefficients
coeffs = lasso_fitted.named_steps['lasso_regression'].coef_

# Formatting coefficients and features in dataframe
comparing_coeffs["Lasso"] = coeffs

print(comparing_coeffs, -gscv.best_score_) # 152082.74

```

```{python}
# Running elastic net regression for the best numerical variables.


# Setting up pipeline
elastic_pipe = Pipeline(
  [("preprocessing", ct),
  ("elastic_regression", ElasticNet(max_iter=1000))]
)

# Defining the alpha and l1 ratio values
tuning = {'elastic_regression__alpha': np.array([1, 0.1, 0.01, 0.001, 0.0001]),
          'elastic_regression__l1_ratio': np.arange(0.2, 1.0, 0.2)}

# Defining the grid search cross validation on the ridge pipeline 
gscv = GridSearchCV(elastic_pipe, param_grid=tuning, cv = 5, scoring='neg_mean_squared_error')

fitted_grid = gscv.fit(X,y)

best_alpha = fitted_grid.best_params_['elastic_regression__alpha']

elastic_pipe = fitted_grid.best_estimator_

# Fitting pipeline to entire dataset
elastic_fitted = elastic_pipe.fit(X,y)

# Getting all the feature names from the fitted column transformer steps - dummify and standardizing variables
features = elastic_fitted.named_steps['preprocessing'].get_feature_names_out()

# Getting all the coefficients
coeffs = elastic_fitted.named_steps['elastic_regression'].coef_

# Formatting coefficients and features in dataframe
comparing_coeffs["elastic"] = coeffs

print(comparing_coeffs, -gscv.best_score_) # 152041.06
```


Using the five best numeric variables and their interactions with the one best categorical variable.

```{python}
# Running linear regression for the best numerical variables.

X = df[['Division', 'Hits', 'RBI', 'AtBat', 'Walks', 'PutOuts']]

# Setting up new column transformer
ct = ColumnTransformer(
    [    # Dummifying league, division, and new league
        ('dummify', OneHotEncoder(sparse_output=False, drop='first', handle_unknown='ignore'), ['Division']),
        ('standardize', StandardScaler(), ['Hits', 'RBI', 'AtBat', 'Walks', 'PutOuts'])
    ],
    remainder='drop'
)

# Setting up pipeline
lr_pipe = Pipeline(
  [("preprocessing", ct),
   ('interaction', PolynomialFeatures(degree=2, include_bias=False, interaction_only=True)),
    # Standardizing all numeric variables
    ('standardize', StandardScaler()),
  ("linear_regression", LinearRegression())]
)

# Fitting pipeline to entire dataset
lr_fitted = lr_pipe.fit(X,y)

# Getting all the feature names from the fitted column transformer steps - dummify and standardizing variables
features = lr_fitted.named_steps['preprocessing'].get_feature_names_out()

# Getting all the coefficients
coeffs = lr_fitted.named_steps['linear_regression'].coef_

# Defining names after categorical and numerical interaction
polynomial = lr_fitted.named_steps['interaction']
polynomial_names = polynomial.get_feature_names_out(features)

# Initializing dataframe to put coefficients in
comparing_coeffs = pd.DataFrame()

# Formatting coefficients and features in dataframe
comparing_coeffs['features'] = polynomial_names
comparing_coeffs["LR"] = coeffs

print(comparing_coeffs)

```

```{python}
# Running ridge regression for the best numerical variables.


# Setting up pipeline
ridge_pipe = Pipeline(
  [("preprocessing", ct),
   ('interaction', PolynomialFeatures(degree=2, include_bias=False, interaction_only=True)),
    # Standardizing all numeric variables
    ('standardize', StandardScaler()),
   ("ridge_regression", Ridge())]
)

# Defining the alpha values
alphas = {'ridge_regression__alpha': np.array([1000, 100, 10, 1, 0.1, 0.01, 0.001, 0.0001])}

# Defining the grid search cross validation on the ridge pipeline 
gscv = GridSearchCV(ridge_pipe, param_grid=alphas, cv = 5, scoring='neg_mean_squared_error')

fitted_grid = gscv.fit(X,y)

best_alpha = fitted_grid.best_params_['ridge_regression__alpha']

ridge_pipe = fitted_grid.best_estimator_

# Fitting pipeline to entire dataset
ridge_fitted = ridge_pipe.fit(X,y)

# Getting all the feature names from the fitted column transformer steps - dummify and standardizing variables
features = ridge_fitted.named_steps['preprocessing'].get_feature_names_out()

# Getting all the coefficients
coeffs = ridge_fitted.named_steps['ridge_regression'].coef_

# Defining names after categorical and numerical interaction
polynomial = ridge_fitted.named_steps['interaction']
polynomial_names = polynomial.get_feature_names_out(features)


# Formatting coefficients and features in dataframe
comparing_coeffs['features'] = polynomial_names
comparing_coeffs["Ridge"] = coeffs

print(comparing_coeffs, -gscv.best_score_) # 145160.99

```

```{python}
# Running lasso regression for the best numerical variables.


# Setting up pipeline
lasso_pipe = Pipeline(
  [("preprocessing", ct),
   ('interaction', PolynomialFeatures(degree=2, include_bias=False, interaction_only=True)),
    # Standardizing all numeric variables
    ('standardize', StandardScaler()),
  ("lasso_regression", Lasso())]
)


# Defining the alpha values
alphas = {'lasso_regression__alpha': np.array([1000, 100, 10, 1, 0.1, 0.01, 0.001, 0.0001])}

# Defining the grid search cross validation on the ridge pipeline 
gscv = GridSearchCV(lasso_pipe, param_grid=alphas, cv = 5, scoring='neg_mean_squared_error')

fitted_grid = gscv.fit(X,y)

best_alpha_lasso_interact = fitted_grid.best_params_['lasso_regression__alpha']

lasso_pipe = fitted_grid.best_estimator_

# Fitting pipeline to entire dataset
lasso_fitted = lasso_pipe.fit(X,y)

# Getting all the feature names from the fitted column transformer steps - dummify and standardizing variables
features = lasso_fitted.named_steps['preprocessing'].get_feature_names_out()

# Getting all the coefficients
coeffs = lasso_fitted.named_steps['lasso_regression'].coef_

# Defining names after categorical and numerical interaction
polynomial = lasso_fitted.named_steps['interaction']
polynomial_names = polynomial.get_feature_names_out(features)


# Formatting coefficients and features in dataframe
comparing_coeffs['features'] = polynomial_names
comparing_coeffs["Lasso"] = coeffs


print(comparing_coeffs, -gscv.best_score_) # 144308.87
```

```{python}
# Running elastic net regression for the best numerical variables.


# Setting up pipeline
elastic_pipe = Pipeline(
  [("preprocessing", ct),
   ('interaction', PolynomialFeatures(degree=2, include_bias=False, interaction_only=True)),
    # Standardizing all numeric variables
    ('standardize', StandardScaler()),
  ("elastic_regression", ElasticNet(max_iter=1000))]
)

# Defining the alpha and l1 ratio values
tuning = {'elastic_regression__alpha': np.array([1, 0.1, 0.01, 0.001, 0.0001]),
          'elastic_regression__l1_ratio': np.arange(0.2, 1.0, 0.2)}

# Defining the grid search cross validation on the ridge pipeline 
gscv = GridSearchCV(elastic_pipe, param_grid=tuning, cv = 5, scoring='neg_mean_squared_error')

fitted_grid = gscv.fit(X,y)

best_alpha = fitted_grid.best_params_['elastic_regression__alpha']

elastic_pipe = fitted_grid.best_estimator_


# Fitting pipeline to entire dataset
elastic_fitted = elastic_pipe.fit(X,y)

# Getting all the feature names from the fitted column transformer steps - dummify and standardizing variables
features = elastic_fitted.named_steps['preprocessing'].get_feature_names_out()

# Getting all the coefficients
coeffs = elastic_fitted.named_steps['elastic_regression'].coef_

# Defining names after categorical and numerical interaction
polynomial = elastic_fitted.named_steps['interaction']
polynomial_names = polynomial.get_feature_names_out(features)

# Formatting coefficients and features in dataframe
comparing_coeffs['features'] = polynomial_names
comparing_coeffs["Elastic"] = coeffs

print(comparing_coeffs, -gscv.best_score_) # 144993.25

```

Report which combination of features and model performed best, based on the validation metric of MSE.
_The lasso model with the five best numeric variables and Division Interactions performed best in part 2, with a cross validation MSE of ~144,309. The elastic net regression model and the ridge regression model followed closely, at 145,161 and 144,993, respectively._

(Note: lambda and alpha must be re-tuned for each feature set.)

Part III. Discussion
NOTE: Cross validation MSEs are reported per feature set after retuning. Single test MSEs reported earlier come from full feature 80/20 split and aren't directly comparable.

A. Ridge
Compare your Ridge models with your ordinary regression models. How did your coefficients compare? Why does this make sense?

_The ridge models produced smaller, more regularized coefficients than the ordinary linear regression model. Overall, this makes sense because Ridge applies an L2 penalty that shrinks large coefficients toward zero, reducing sensitivity to multicollinearity. Compared to the baseline OLS MSE of about 112,677, ridge achieved a comparable test MSE of around 126,552 and a lower cross-validation MSE of ~145,161 in the reduced feature set with interactions. This trade off with slightly higher bias for reduced variance results in more stable predictions across different subsets of the data._

B. LASSO
Compare your LASSO model in I with your three LASSO models in II. Did you get the same 
 results? Why does this make sense? Did you get the same MSEs? Why does this make sense?

_The Lasso models in part 2 led to sparser solutions than in part 1, with several of the coefficients shrunk exactly to 0. This aligns with Lasso's L1 regularization, which performs variable selection in addition to shrinkage. The part 1 lasso had an MSE of 126,212, while the best model, which used the five key numeric variables and Division interactions, achieved a remarkably lower cross validation MSE of ~144,309, the lowest across all models. This improvement reflects how Lasso effectively removed uninformative predictors and optimized the bias-variance balance, enhancing predictive accuracy and interpretability._

C. Elastic Net
Compare your MSEs for the Elastic Net models with those for the Ridge and LASSO models. Why does it make sense that Elastic Net always “wins”?

_Elastic Net's performance consistently fell between Ridge and Lasso, combining the strengths of both penalties. In the final interaction model, Elastic Net achieved a cross validation MSE of ~144,993, slightly higher than Lasso, which was around 144,309. This was lower than Ridge, which was about 145,161. This makes sense, because Elastic Net takes L1's feature selection ability with L2's coefficient stability, making it effective when predictors are correlated. The balance between sparsity and smooth shrinkage allowed Elastic Net to perform robustly across all feature configurations._

Part IV: Final Model
Fit your final best pipeline on the full dataset, and summarize your results in a few short sentences and a plot.


```{python}
# Fitting best pipeline to the full dataset - Lasso with 5 best numeric variables and Division interaction

# Defining X and y
X = df[['Division', 'Hits', 'RBI', 'AtBat', 'Walks', 'PutOuts']]
y = df['Salary']

# Setting up Column Transformer
ct = ColumnTransformer(
    [    # Dummifying league, division, and new league
        ('dummify', OneHotEncoder(sparse_output=False, drop='first', handle_unknown='ignore'), ['Division']),
        ('standardize', StandardScaler(), ['Hits', 'RBI', 'AtBat', 'Walks', 'PutOuts'])
    ],
    remainder='drop'
)

# Setting up final lasso pipeline
lasso_pipe = Pipeline(
  [("preprocessing", ct),
   ('interaction', PolynomialFeatures(degree=2, include_bias=False, interaction_only=True)),
    # Standardizing all numeric variables
    ('standardize', StandardScaler()),
  ("lasso_regression", Lasso(alpha=best_alpha_lasso_interact))]
)

# Fitting the model to the full dataset
final_lasso_fitted = lasso_pipe.fit(X,y)

# Extracting the feature names and coeffs
features = final_lasso_fitted.named_steps['preprocessing'].get_feature_names_out()
poly = final_lasso_fitted.named_steps['interaction']
poly_names = poly.get_feature_names_out(features)
coeffs = final_lasso_fitted.named_steps['lasso_regression'].coef_

# Creating a df of coeffs
final_coeffs = pd.DataFrame({'Feature': poly_names, 'Coefficient': coeffs}).sort_values('Coefficient', ascending=False)

final_coeffs
```


```{python}
from plotnine import *
# Plotting the top coefficients

# Getting the top 10 coefficients from the dataframe
top10 = final_coeffs.head(10)

# Creating ggplot bar plot of the coefficients
(
    ggplot(top10, aes(x='Feature', y='Coefficient', fill='Coefficient'))
    + geom_bar(stat='identity')
    + coord_flip()
    + labs(
        title='Top 10 Coefficients in the Final Lasso Model',
        x='Feature',
        y='Coefficient value'
    )
    + theme_minimal()
)
```

Interpretation: The final model uses a lasso regression with the 5 most important numeric variables (Hits, RBI, AtBat, Walks, PutOuts) and their interactions with Division. Standardization followed the interaction expansion to balance the coefficient magnitudes. The top predictors in the final model include combinations such as standardize_hits, standardize_Hits * standardize_Walks, and standardize walks, each having a positive effects on salary (in thousands). Overall, this indicates that higher hitting and offensive stats, especially and being in the West Division, are the most influential in determining salary. Conclusively, we can see that regulaization improved the predictive stability while maintaining interpretability.